{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS via Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret linear regression as an extremely simple feedforward neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "The data generating process (DGP) simulates a linear relationship with three independent variables. \n",
    "\n",
    "1.  **Sample Size ($n$)**: 100 observations.\n",
    "2.  **Independent Variables ($x_1, x_2, x_3$)**: Drawn independently from a standard normal distribution, $\\mathcal{N}(0, 1)$.\n",
    "3.  **Parameters**:\n",
    "    *   Intercept: $\\beta_0 = 0.5$\n",
    "    *   Coefficients: $\\beta_1 = 1$, $\\beta_2 = -1$, $\\beta_3 = 1$\n",
    "4.  **Noise ($\\epsilon$)**: The error term follows a standard normal distribution, $\\epsilon \\sim \\mathcal{N}(0, 1)$.\n",
    "5.  **Dependent Variable ($y$)**: Generated according to the equation $y = 0.5 + 1x_1 - 1x_2 + 1x_3 + \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data with three regressors\n",
    "n = 100  # number of observations\n",
    "x1 = np.random.normal(0, 1, n)  # first explanatory variable\n",
    "x2 = np.random.normal(0, 1, n)  # second explanatory variable\n",
    "x3 = np.random.normal(0, 1, n)  # third explanatory variable\n",
    "beta_0 = 0.5  # intercept\n",
    "beta_1 = 1.  # coefficient for x1\n",
    "beta_2 = -1.  # coefficient for x2\n",
    "beta_3 = 1.  # coefficient for x3\n",
    "epsilon = np.random.normal(0, 1, n)  # error term with standard normal distribution\n",
    "y = beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + epsilon  # dependent variable\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'y': y})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sm.OLS()` as in statistics / econometrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression Results Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.817\n",
      "Model:                            OLS   Adj. R-squared:                  0.811\n",
      "Method:                 Least Squares   F-statistic:                     142.8\n",
      "Date:                Sun, 18 Jan 2026   Prob (F-statistic):           2.84e-35\n",
      "Time:                        22:04:17   Log-Likelihood:                -127.46\n",
      "No. Observations:                 100   AIC:                             262.9\n",
      "Df Residuals:                      96   BIC:                             273.3\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5875      0.089      6.584      0.000       0.410       0.765\n",
      "x1             0.8227      0.100      8.188      0.000       0.623       1.022\n",
      "x2            -1.0382      0.094    -11.047      0.000      -1.225      -0.852\n",
      "x3             1.0269      0.083     12.309      0.000       0.861       1.193\n",
      "==============================================================================\n",
      "Omnibus:                        1.353   Durbin-Watson:                   1.821\n",
      "Prob(Omnibus):                  0.508   Jarque-Bera (JB):                1.317\n",
      "Skew:                           0.169   Prob(JB):                        0.518\n",
      "Kurtosis:                       2.551   Cond. No.                         1.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Fit OLS regression using statsmodels\n",
    "# produce an X with `x1`, x2`, and `x3` in data\n",
    "X = data[['x1', 'x2', 'x3']]\n",
    "X = sm.add_constant(X)  # add a constant term to the model\n",
    "y = data['y']\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(\"OLS Regression Results Summary:\")\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `pytorch` implemenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_tensor = torch.tensor(data[['x1', 'x2', 'x3']].values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(data['y'].values.reshape(-1, 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network with one hidden layer\n",
    "class OLS_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OLS_NN, self).__init__()\n",
    "        self.output = nn.Linear(3, 1)  # three inputs, one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(x)  # identity function as activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = OLS_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/1000, Loss: 0.7498\n",
      "Epoch 400/1000, Loss: 0.7493\n",
      "Epoch 600/1000, Loss: 0.7493\n",
      "Epoch 800/1000, Loss: 0.7493\n",
      "Epoch 1000/1000, Loss: 0.7493\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights after training:\n",
      "output.weight: [[ 0.8226794 -1.0382409  1.0268975]]\n",
      "output.bias: [0.58753484]\n"
     ]
    }
   ],
   "source": [
    "# Report the weights after training\n",
    "print(\"\\nWeights after training:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients from NN are the same as from OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
