
# Frish-Waugh-Lovell Theorem

Let $X$ be a $p$-dimensional random vector, and $Y$ be a random scalar. They live in a joint probability space with measure $\mu$. 

## Geometry of linear projection (Hilbert space language)

### 1) The ambient Hilbert space
Work in the real Hilbert space
$$
\mathcal H := L^2(\mu) = \{Z: \Omega\to\mathbb R \text{ measurable},\ \mathbb E[Z^2]<\infty\},
$$
equipped with inner product and norm
$$
\langle Z_1,Z_2\rangle := \mathbb E[Z_1Z_2],\qquad \|Z\| := \sqrt{\langle Z,Z\rangle}.
$$
Geometrically, elements of $\mathcal H$ are “vectors”, and squared distance is
$$
\|Y-Z\|^2 = \mathbb E[(Y-Z)^2].
$$

### 2) The linear space spanned by $X$
Let $X=(X_1,\dots,X_p)^\top$. Consider the (closed) linear subspace generated by the components of $X$ (and optionally an intercept):
$$
\mathcal S := \overline{\mathrm{span}\{X_1,\dots,X_p\}}\subseteq \mathcal H.
$$
Every element of $\mathrm{span}\{X_1,\dots,X_p\}$ is a linear combination $X^\top\beta$.

### 3) Projection theorem and the “best linear predictor”
The **linear projection** of $Y$ onto $\mathcal S$ is the unique element $\Pi_{\mathcal S}Y\in \mathcal S$ that minimizes the squared distance to $Y$:
$$
\Pi_{\mathcal S}Y \;:=\; \arg\min_{Z\in \mathcal S}\|Y-Z\|^2.
$$
If we restrict attention to $Z=X^\top\beta$, this is the same as
$$
\beta^* \in \arg\min_{\beta\in\mathbb R^p}\mathbb E\big[(Y-X^\top\beta)^2\big],
\qquad \widehat Y:=X^\top\beta^* = \Pi_{\mathcal S}Y.
$$
Hilbert space geometry says the minimizer is characterized by **orthogonality of the residual**:
$$
U := Y-\widehat Y \perp \mathcal S.
$$
Equivalently, $U$ is orthogonal to every direction in the subspace, in particular to each regressor:
$$
\langle U, X_j\rangle = \mathbb E[U X_j]=0\quad (j=1,\dots,p).
$$
These are exactly the **normal equations**
$$
\mathbb E\big[X(Y-X^\top\beta^*)\big]=0
\quad\Longleftrightarrow\quad
\mathbb E[XX^\top]\,\beta^* = \mathbb E[XY].
$$
When $\mathbb E[XX^\top]$ is invertible,
$$
\beta^* = \big(\mathbb E[XX^\top]\big)^{-1}\mathbb E[XY].
$$

### 4) Right angle picture and Pythagoras
Because $\widehat Y\in\mathcal S$ and $U\perp \mathcal S$, we have the orthogonal decomposition
$$
Y = \widehat Y + U,\qquad \widehat Y\in\mathcal S,\ U\in\mathcal S^{\perp}.
$$
This gives a Pythagorean identity:
$$
\|Y\|^2 = \|\widehat Y\|^2 + \|U\|^2,
$$
and for any other $Z\in\mathcal S$,
$$
\|Y-Z\|^2 = \|Y-\widehat Y\|^2 + \|\widehat Y-Z\|^2
\ \ge\ \|Y-\widehat Y\|^2.
$$
So $\widehat Y$ is the closest point in the subspace to $Y$.

### 5) Finite-sample geometry (column space of a matrix)
In a sample of size $n$, let $y\in\mathbb R^n$ and $X\in\mathbb R^{n\times p}$. Then the geometry is the same in the Euclidean Hilbert space $(\mathbb R^n,\langle a,b\rangle=a^\top b)$:
$$
\widehat y = \arg\min_{z\in\mathrm{col}(X)}\|y-z\|^2.
$$
Writing $\widehat y = X\widehat\beta$, the orthogonality condition becomes
$$
X^\top(y-X\widehat\beta)=0,
$$
and the projection and residual-maker matrices are
$$
P_X := X(X^\top X)^{-1}X^\top,\qquad M_X := I - P_X,
$$
so that
$$
\widehat y = P_X y,\qquad \widehat u = M_X y,\qquad X^\top\widehat u = 0.
$$
Geometrically, $P_Xy$ is the “shadow” of $y$ on the column space of $X$, and $M_Xy$ is the perpendicular component.

### 6) Why this matters for FWL
FWL is ultimately a statement about **projections onto subspaces and their orthogonal complements**: when you “partial out” controls $W$, you are applying the residual-maker $M_W$ (projection onto $\mathrm{col}(W)^{\perp}$). Regressing residualized outcomes on residualized regressors is exactly working in the orthogonal complement subspace where the controls have been removed.

## Frisch–Waugh–Lovell theorem (Hilbert space version)

Let $X=(X_1,X_2)$ where $X_1\in \mathcal H$ is a scalar regressor and $X_2\in \mathcal H^{p-1}$ is a vector of controls. Consider the linear projection of $Y$ onto the closed subspace
$$
\mathcal S := \overline{\mathrm{span}\{X_1, X_2\}}\subseteq \mathcal H,
$$
so that
$$
Y = X_1\beta_1 + X_2'\beta_2 + U,\qquad U \perp \mathcal S.
$$

Define the control subspace $\mathcal S_2:=\overline{\mathrm{span}\{X_2\}}$ and the orthogonal projection $\Pi_2:=\Pi_{\mathcal S_2}$. The associated “partialling-out” (residual-making) operator is
$$
M_2 := I-\Pi_2,\qquad M_2Z = Z-\Pi_2 Z \in \mathcal S_2^{\perp}.
$$
In particular, define the residualized variables
$$
\widetilde Y := M_2 Y,\qquad \widetilde X_1 := M_2 X_1.
$$

### Theorem (FWL)
The coefficient $\beta_1$ in the projection of $Y$ on $(X_1,X_2)$ equals the coefficient from projecting $\widetilde Y$ on $\widetilde X_1$:
$$
\beta_1
= \arg\min_{b\in\mathbb R}\,\|\widetilde Y - \widetilde X_1 b\|^2.
$$
Equivalently (when $\langle \widetilde X_1,\widetilde X_1\rangle>0$),
$$
\beta_1 = \frac{\langle \widetilde X_1,\widetilde Y\rangle}{\langle \widetilde X_1,\widetilde X_1\rangle}
= \frac{\mathbb E[\widetilde X_1\,\widetilde Y]}{\mathbb E[\widetilde X_1^2]}.
$$

### Proof (geometry)
Start from the defining orthogonality conditions for the projection on $\mathcal S$:
$$
\langle U, X_1\rangle = 0,\qquad \langle U, Z\rangle=0\ \text{for all } Z\in \mathcal S_2.
$$
The second condition says exactly that $U\in \mathcal S_2^{\perp}$, hence $\Pi_2 U = 0$ and $M_2U=U$.

Apply $M_2$ to the decomposition $Y = X_1\beta_1 + X_2'\beta_2 + U$. Using $M_2X_2=0$ (because $X_2\in\mathcal S_2$) and $M_2U=U$, we get
$$
\widetilde Y = M_2Y = \beta_1 M_2X_1 + U = \beta_1\widetilde X_1 + U.
$$
Now take inner products with $\widetilde X_1$.
Because $\widetilde X_1\in \mathcal S_2^{\perp}$ and $U\in\mathcal S^{\perp}\subseteq\mathcal S_2^{\perp}$, the projection condition $\langle U, X_1\rangle=0$ is equivalent to $\langle U,\widetilde X_1\rangle=0$ (since $X_1-\widetilde X_1=\Pi_2X_1\in\mathcal S_2$ and $U\perp\mathcal S_2$). Therefore
$$
\langle \widetilde X_1,\widetilde Y\rangle
= \beta_1\langle \widetilde X_1,\widetilde X_1\rangle,
$$
which yields $\beta_1 = \langle \widetilde X_1,\widetilde Y\rangle/\langle \widetilde X_1,\widetilde X_1\rangle$.
This is exactly the normal equation for the one-regressor projection of $\widetilde Y$ onto $\mathrm{span}\{\widetilde X_1\}$.

### Finite-sample form
With a sample, let $X=[x_1\ \ X_2]$ and define $M_2 := I-P_{X_2}$. Then FWL says the coefficient on $x_1$ in OLS of $y$ on $(x_1,X_2)$ equals the coefficient from OLS of $M_2y$ on $M_2x_1$:
$$
\widehat\beta_1 = \frac{(M_2x_1)^\top(M_2y)}{(M_2x_1)^\top(M_2x_1)}.
$$