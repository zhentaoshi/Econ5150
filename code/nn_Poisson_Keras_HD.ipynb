{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Poisson Progression using TensorFlow in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: Data\n",
    "\n",
    "Outputs: A model and b_hat\n",
    "\n",
    "The environment should have these packaages: `tensorflow`, `keras`, `pandas`, `sklearn`, `numpy`, `matplotlib `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(898)\n",
    "nn = 100000\n",
    "K = 100\n",
    "\n",
    "X = np.random.uniform(size=(nn, K))\n",
    "b0 = np.ones(K) / K\n",
    "y = np.random.poisson(np.exp(X @ b0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 100), (100000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X), np.shape(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide up the dataset into training and validation data. `test_size` selects a portion of data to be test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array(X,dtype=\"float32\")\n",
    "y=np.array(y,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up a neural network model with a single dense layer using the exponential activation function. \n",
    "\n",
    "- `Sequential` is a Keras class that allows us to create models layer by layer.\n",
    "- `1` means the output has one dimension.\n",
    "- `input_dim=X_train.shape[1]`: This specifies the dimensionality of the input data. It's set to the number of features in the input data X_train.\n",
    "- `activation='exponential'`: This specifies the activation function for the layer. In this case, it's set to 'exponential', which means the output of the layer will be the exponential of the weighted sum of inputs.\n",
    "`output = activation(dot(input, kernel) + bias) = exp(dot(input, kernel) + bias)`\n",
    "\n",
    "\n",
    "Find more information in \n",
    "[Dense layer](https://keras.io/2.15/api/layers/core_layers/dense/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "  \n",
    "model = Sequential() \n",
    "model.add(Dense(1, input_dim=X_train.shape[1], activation='exponential'))  \n",
    "# exponential activation function\n",
    "# this is about the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show infomation about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code configures the neural network model for training using the Adam optimizer, the Poisson loss function, and the Poisson metric for evaluation.\n",
    "\n",
    "- `optimizer=Adam()`: This parameter specifies the optimizer to be used during training. In this case, it's set to `Adam()`, whihch is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "-  `loss=keras.losses.Poisson()`: This parameter specifies the loss function to be used during training. The `Poisson()` function here refers to the Poisson loss function, which is `loss = mean(sum(y_pred - y_true * log(y_pred)))`\n",
    "- `metrics=[keras.metrics.Poisson()]`: This parameter specifies the evaluation metrics to be used during training. It could be a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam  \n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=keras.losses.Poisson(), \n",
    "    metrics=[keras.metrics.Poisson()],\n",
    "    # this metrics.Poisson is about the validation data\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `fit()`, which will train the model by slicing the data into \"batches\" of size\n",
    "`batch_size`, and repeatedly iterating over the entire dataset for a given number of\n",
    "`epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/300\n",
      "2125/2125 [==============================] - 6s 2ms/step - loss: 0.8682 - poisson: 0.8682 - val_loss: 0.8296 - val_poisson: 0.8296\n",
      "Epoch 2/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8278 - poisson: 0.8278 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 3/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8380 - val_poisson: 0.8380\n",
      "Epoch 4/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 5/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 6/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 7/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 8/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 9/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8393 - val_poisson: 0.8393\n",
      "Epoch 10/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8276 - val_poisson: 0.8276\n",
      "Epoch 11/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 12/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8281 - val_poisson: 0.8281\n",
      "Epoch 13/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 14/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8295 - val_poisson: 0.8295\n",
      "Epoch 15/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 16/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 17/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 18/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8317 - val_poisson: 0.8317\n",
      "Epoch 19/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 20/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8330 - val_poisson: 0.8330\n",
      "Epoch 21/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8258 - val_poisson: 0.8258\n",
      "Epoch 22/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8275 - val_poisson: 0.8275\n",
      "Epoch 23/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8302 - val_poisson: 0.8302\n",
      "Epoch 24/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8339 - val_poisson: 0.8339\n",
      "Epoch 25/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 26/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8290 - val_poisson: 0.8290\n",
      "Epoch 27/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 28/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 29/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 30/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 31/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8259 - val_poisson: 0.8259\n",
      "Epoch 32/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8290 - val_poisson: 0.8290\n",
      "Epoch 33/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8308 - val_poisson: 0.8308\n",
      "Epoch 34/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8259 - val_poisson: 0.8259\n",
      "Epoch 35/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8308 - val_poisson: 0.8308\n",
      "Epoch 36/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 37/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8292 - val_poisson: 0.8292\n",
      "Epoch 38/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8336 - val_poisson: 0.8336\n",
      "Epoch 39/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8258 - val_poisson: 0.8258\n",
      "Epoch 40/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8326 - val_poisson: 0.8326\n",
      "Epoch 41/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 42/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 43/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 44/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8319 - val_poisson: 0.8319\n",
      "Epoch 45/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 46/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8259 - val_poisson: 0.8259\n",
      "Epoch 47/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8277 - poisson: 0.8277 - val_loss: 0.8257 - val_poisson: 0.8257\n",
      "Epoch 48/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8277 - val_poisson: 0.8277\n",
      "Epoch 49/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8278 - val_poisson: 0.8278\n",
      "Epoch 50/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8271 - val_poisson: 0.8271\n",
      "Epoch 51/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8312 - val_poisson: 0.8312\n",
      "Epoch 52/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 53/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8324 - val_poisson: 0.8324\n",
      "Epoch 54/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 55/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 56/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8302 - val_poisson: 0.8302\n",
      "Epoch 57/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8316 - val_poisson: 0.8316\n",
      "Epoch 58/300\n",
      "2125/2125 [==============================] - 4s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8286 - val_poisson: 0.8286\n",
      "Epoch 59/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8300 - val_poisson: 0.8300\n",
      "Epoch 60/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8429 - val_poisson: 0.8429\n",
      "Epoch 61/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8316 - val_poisson: 0.8316\n",
      "Epoch 62/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8284 - val_poisson: 0.8284\n",
      "Epoch 63/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 64/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8263 - poisson: 0.8263 - val_loss: 0.8277 - val_poisson: 0.8277\n",
      "Epoch 65/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 66/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8283 - val_poisson: 0.8283\n",
      "Epoch 67/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 68/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 69/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8302 - val_poisson: 0.8302\n",
      "Epoch 70/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8276 - val_poisson: 0.8276\n",
      "Epoch 71/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8256 - val_poisson: 0.8256\n",
      "Epoch 72/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 73/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 74/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8273 - val_poisson: 0.8273\n",
      "Epoch 75/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 76/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8258 - val_poisson: 0.8258\n",
      "Epoch 77/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 78/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 79/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 80/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 81/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 82/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8288 - val_poisson: 0.8288\n",
      "Epoch 83/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 84/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 85/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8285 - val_poisson: 0.8285\n",
      "Epoch 86/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8359 - val_poisson: 0.8359\n",
      "Epoch 87/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8313 - val_poisson: 0.8313\n",
      "Epoch 88/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8302 - val_poisson: 0.8302\n",
      "Epoch 89/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 90/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 91/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 92/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8314 - val_poisson: 0.8314\n",
      "Epoch 93/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8286 - val_poisson: 0.8286\n",
      "Epoch 94/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8300 - val_poisson: 0.8300\n",
      "Epoch 95/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8287 - val_poisson: 0.8287\n",
      "Epoch 96/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 97/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8324 - val_poisson: 0.8324\n",
      "Epoch 98/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8325 - val_poisson: 0.8325\n",
      "Epoch 99/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8343 - val_poisson: 0.8343\n",
      "Epoch 100/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 101/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8360 - val_poisson: 0.8360\n",
      "Epoch 102/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8345 - val_poisson: 0.8345\n",
      "Epoch 103/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 104/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 105/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 106/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8274 - val_poisson: 0.8274\n",
      "Epoch 107/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 108/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8273 - val_poisson: 0.8273\n",
      "Epoch 109/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 110/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8346 - val_poisson: 0.8346\n",
      "Epoch 111/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8296 - val_poisson: 0.8296\n",
      "Epoch 112/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 113/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 114/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8259 - val_poisson: 0.8259\n",
      "Epoch 115/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 116/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8268 - val_poisson: 0.8268\n",
      "Epoch 117/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 118/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 119/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8284 - val_poisson: 0.8284\n",
      "Epoch 120/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 121/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 122/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8325 - val_poisson: 0.8325\n",
      "Epoch 123/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 124/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 125/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 126/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 127/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 128/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 129/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8374 - val_poisson: 0.8374\n",
      "Epoch 130/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 131/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8278 - val_poisson: 0.8278\n",
      "Epoch 132/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8273 - val_poisson: 0.8273\n",
      "Epoch 133/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8342 - val_poisson: 0.8342\n",
      "Epoch 134/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8275 - val_poisson: 0.8275\n",
      "Epoch 135/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 136/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 137/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 138/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 139/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8306 - val_poisson: 0.8306\n",
      "Epoch 140/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 141/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 142/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 143/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8268 - val_poisson: 0.8268\n",
      "Epoch 144/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 145/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8258 - val_poisson: 0.8258\n",
      "Epoch 146/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8273 - val_poisson: 0.8273\n",
      "Epoch 147/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 148/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8275 - val_poisson: 0.8275\n",
      "Epoch 149/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 150/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 151/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8310 - val_poisson: 0.8310\n",
      "Epoch 152/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 153/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8286 - val_poisson: 0.8286\n",
      "Epoch 154/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8348 - val_poisson: 0.8348\n",
      "Epoch 155/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8348 - val_poisson: 0.8348\n",
      "Epoch 156/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8323 - val_poisson: 0.8323\n",
      "Epoch 157/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8297 - val_poisson: 0.8297\n",
      "Epoch 158/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8286 - val_poisson: 0.8286\n",
      "Epoch 159/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8324 - val_poisson: 0.8324\n",
      "Epoch 160/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8315 - val_poisson: 0.8315\n",
      "Epoch 161/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 162/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8298 - val_poisson: 0.8298\n",
      "Epoch 163/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8271 - val_poisson: 0.8271\n",
      "Epoch 164/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 165/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8283 - val_poisson: 0.8283\n",
      "Epoch 166/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 167/300\n",
      "2125/2125 [==============================] - 10s 4ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8338 - val_poisson: 0.8338\n",
      "Epoch 168/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8303 - val_poisson: 0.8303\n",
      "Epoch 169/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 170/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 171/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8366 - val_poisson: 0.8366\n",
      "Epoch 172/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8475 - val_poisson: 0.8475\n",
      "Epoch 173/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8396 - val_poisson: 0.8396\n",
      "Epoch 174/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 175/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8270 - val_poisson: 0.8270\n",
      "Epoch 176/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8258 - val_poisson: 0.8258\n",
      "Epoch 177/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8274 - val_poisson: 0.8274\n",
      "Epoch 178/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 179/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 180/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8303 - val_poisson: 0.8303\n",
      "Epoch 181/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8370 - val_poisson: 0.8370\n",
      "Epoch 182/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 183/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 184/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 185/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 186/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8273 - poisson: 0.8273 - val_loss: 0.8301 - val_poisson: 0.8301\n",
      "Epoch 187/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 188/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8275 - val_poisson: 0.8275\n",
      "Epoch 189/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 190/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 191/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 192/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 193/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 194/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8306 - val_poisson: 0.8306\n",
      "Epoch 195/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 196/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8316 - val_poisson: 0.8316\n",
      "Epoch 197/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 198/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 199/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8262 - poisson: 0.8262 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 200/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8273 - poisson: 0.8273 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 201/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8337 - val_poisson: 0.8337\n",
      "Epoch 202/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 203/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 204/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8319 - val_poisson: 0.8319\n",
      "Epoch 205/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8281 - val_poisson: 0.8281\n",
      "Epoch 206/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 207/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8307 - val_poisson: 0.8307\n",
      "Epoch 208/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8273 - val_poisson: 0.8273\n",
      "Epoch 209/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 210/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8317 - val_poisson: 0.8317\n",
      "Epoch 211/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8284 - val_poisson: 0.8284\n",
      "Epoch 212/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 213/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 214/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 215/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 216/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8323 - val_poisson: 0.8323\n",
      "Epoch 217/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 218/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8289 - val_poisson: 0.8289\n",
      "Epoch 219/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 220/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8271 - val_poisson: 0.8271\n",
      "Epoch 221/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 222/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8276 - poisson: 0.8276 - val_loss: 0.8269 - val_poisson: 0.8269\n",
      "Epoch 223/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 224/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 225/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8276 - poisson: 0.8276 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 226/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8287 - val_poisson: 0.8287\n",
      "Epoch 227/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 228/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8285 - val_poisson: 0.8285\n",
      "Epoch 229/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8297 - val_poisson: 0.8297\n",
      "Epoch 230/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8281 - val_poisson: 0.8281\n",
      "Epoch 231/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8268 - val_poisson: 0.8268\n",
      "Epoch 232/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 233/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8309 - val_poisson: 0.8309\n",
      "Epoch 234/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8276 - val_poisson: 0.8276\n",
      "Epoch 235/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 236/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8282 - val_poisson: 0.8282\n",
      "Epoch 237/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 238/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8294 - val_poisson: 0.8294\n",
      "Epoch 239/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8277 - val_poisson: 0.8277\n",
      "Epoch 240/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 241/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8290 - val_poisson: 0.8290\n",
      "Epoch 242/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 243/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 244/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8275 - poisson: 0.8275 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 245/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 246/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8297 - val_poisson: 0.8297\n",
      "Epoch 247/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8268 - val_poisson: 0.8268\n",
      "Epoch 248/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8283 - val_poisson: 0.8283\n",
      "Epoch 249/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 250/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 251/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 252/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8399 - val_poisson: 0.8399\n",
      "Epoch 253/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8267 - val_poisson: 0.8267\n",
      "Epoch 254/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8262 - val_poisson: 0.8262\n",
      "Epoch 255/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8343 - val_poisson: 0.8343\n",
      "Epoch 256/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8351 - val_poisson: 0.8351\n",
      "Epoch 257/300\n",
      "2125/2125 [==============================] - 12s 5ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8257 - val_poisson: 0.8257\n",
      "Epoch 258/300\n",
      "2125/2125 [==============================] - 11s 5ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8279 - val_poisson: 0.8279\n",
      "Epoch 259/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8259 - val_poisson: 0.8259\n",
      "Epoch 260/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8286 - val_poisson: 0.8286\n",
      "Epoch 261/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 262/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 263/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 264/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8264 - val_poisson: 0.8264\n",
      "Epoch 265/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8359 - val_poisson: 0.8359\n",
      "Epoch 266/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8378 - val_poisson: 0.8378\n",
      "Epoch 267/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8280 - val_poisson: 0.8280\n",
      "Epoch 268/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8276 - val_poisson: 0.8276\n",
      "Epoch 269/300\n",
      "2125/2125 [==============================] - 5s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 270/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 271/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8274 - poisson: 0.8274 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 272/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8307 - val_poisson: 0.8307\n",
      "Epoch 273/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8272 - poisson: 0.8272 - val_loss: 0.8274 - val_poisson: 0.8274\n",
      "Epoch 274/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8332 - val_poisson: 0.8332\n",
      "Epoch 275/300\n",
      "2125/2125 [==============================] - 11s 5ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8283 - val_poisson: 0.8283\n",
      "Epoch 276/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8261 - val_poisson: 0.8261\n",
      "Epoch 277/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8260 - val_poisson: 0.8260\n",
      "Epoch 278/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8283 - val_poisson: 0.8283\n",
      "Epoch 279/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8277 - val_poisson: 0.8277\n",
      "Epoch 280/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8278 - val_poisson: 0.8278\n",
      "Epoch 281/300\n",
      "2125/2125 [==============================] - 7s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8266 - val_poisson: 0.8266\n",
      "Epoch 282/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8298 - val_poisson: 0.8298\n",
      "Epoch 283/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8272 - val_poisson: 0.8272\n",
      "Epoch 284/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8264 - poisson: 0.8264 - val_loss: 0.8263 - val_poisson: 0.8263\n",
      "Epoch 285/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8274 - val_poisson: 0.8274\n",
      "Epoch 286/300\n",
      "2125/2125 [==============================] - 7s 3ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8312 - val_poisson: 0.8312\n",
      "Epoch 287/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8275 - val_poisson: 0.8275\n",
      "Epoch 288/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8293 - val_poisson: 0.8293\n",
      "Epoch 289/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8311 - val_poisson: 0.8311\n",
      "Epoch 290/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8265 - val_poisson: 0.8265\n",
      "Epoch 291/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8301 - val_poisson: 0.8301\n",
      "Epoch 292/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8329 - val_poisson: 0.8329\n",
      "Epoch 293/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8269 - poisson: 0.8269 - val_loss: 0.8284 - val_poisson: 0.8284\n",
      "Epoch 294/300\n",
      "2125/2125 [==============================] - 5s 2ms/step - loss: 0.8266 - poisson: 0.8266 - val_loss: 0.8278 - val_poisson: 0.8278\n",
      "Epoch 295/300\n",
      "2125/2125 [==============================] - 10s 5ms/step - loss: 0.8271 - poisson: 0.8271 - val_loss: 0.8274 - val_poisson: 0.8274\n",
      "Epoch 296/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8268 - poisson: 0.8268 - val_loss: 0.8323 - val_poisson: 0.8323\n",
      "Epoch 297/300\n",
      "2125/2125 [==============================] - 6s 3ms/step - loss: 0.8270 - poisson: 0.8270 - val_loss: 0.8308 - val_poisson: 0.8308\n",
      "Epoch 298/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8273 - poisson: 0.8273 - val_loss: 0.8308 - val_poisson: 0.8308\n",
      "Epoch 299/300\n",
      "2125/2125 [==============================] - 8s 4ms/step - loss: 0.8267 - poisson: 0.8267 - val_loss: 0.8292 - val_poisson: 0.8292\n",
      "Epoch 300/300\n",
      "2125/2125 [==============================] - 9s 4ms/step - loss: 0.8265 - poisson: 0.8265 - val_loss: 0.8297 - val_poisson: 0.8297\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,    \n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `history` object holds a record of the loss values and metric values\n",
    "during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.8681961894035339, 0.8277866840362549, 0.8266697525978088, 0.8271215558052063, 0.8265001773834229, 0.8269814252853394, 0.8267329335212708, 0.8268933892250061, 0.8270233869552612, 0.8266149163246155, 0.827154815196991, 0.8271946907043457, 0.826579749584198, 0.8268014192581177, 0.8268925547599792, 0.8267494440078735, 0.8270748257637024, 0.826631486415863, 0.8267567753791809, 0.8267937302589417, 0.8268844485282898, 0.8266299366950989, 0.8267364501953125, 0.8270810842514038, 0.8266992568969727, 0.8268337249755859, 0.8268970847129822, 0.8269334435462952, 0.8265900015830994, 0.826556921005249, 0.8270639777183533, 0.8270509243011475, 0.8265610933303833, 0.8267791867256165, 0.826957106590271, 0.8270310759544373, 0.8265109062194824, 0.8269197344779968, 0.8266429305076599, 0.8265719413757324, 0.8265510201454163, 0.8269957304000854, 0.8269032835960388, 0.8269296884536743, 0.8271241784095764, 0.8266963362693787, 0.8276617527008057, 0.826846718788147, 0.826876163482666, 0.8271520137786865, 0.8267638683319092, 0.8266553282737732, 0.8273570537567139, 0.8271679878234863, 0.826922595500946, 0.8268201351165771, 0.8270400166511536, 0.8270386457443237, 0.8268786072731018, 0.8270329236984253, 0.8266760110855103, 0.8269174695014954, 0.8270733952522278, 0.8263143301010132, 0.8271083831787109, 0.8267224431037903, 0.8267468810081482, 0.8266358971595764, 0.8266815543174744, 0.8266082406044006, 0.8270987868309021, 0.8266881704330444, 0.8269663453102112, 0.8267425894737244, 0.8270224332809448, 0.8269215822219849, 0.827211320400238, 0.82683265209198, 0.8267722129821777, 0.8270087838172913, 0.8270189166069031, 0.8266458511352539, 0.8269816637039185, 0.826474666595459, 0.8264777064323425, 0.8271064162254333, 0.8269944787025452, 0.826701283454895, 0.827031135559082, 0.8270744681358337, 0.8271805047988892, 0.8266415596008301, 0.8267378807067871, 0.8269835710525513, 0.8268670439720154, 0.8270387053489685, 0.8268299698829651, 0.827354907989502, 0.8267204761505127, 0.8265712261199951, 0.8265804052352905, 0.827356219291687, 0.8270542025566101, 0.826755940914154, 0.8269258737564087, 0.8263602256774902, 0.8268926739692688, 0.8269543051719666, 0.8269262909889221, 0.8266429305076599, 0.8267120718955994, 0.8267315030097961, 0.8268006443977356, 0.8268387913703918, 0.8267667293548584, 0.8270542621612549, 0.8268669247627258, 0.8269286751747131, 0.8269023299217224, 0.8270446062088013, 0.8266289234161377, 0.8266321420669556, 0.8271960020065308, 0.8269605040550232, 0.8266164660453796, 0.826697051525116, 0.8268269300460815, 0.8266709446907043, 0.8266257047653198, 0.8271938562393188, 0.8265153765678406, 0.8268263936042786, 0.8268808722496033, 0.8274499773979187, 0.8264982104301453, 0.8268078565597534, 0.8269666433334351, 0.8263806700706482, 0.8271584510803223, 0.826622486114502, 0.8266633749008179, 0.8272151947021484, 0.8269597291946411, 0.8269526958465576, 0.8268177509307861, 0.8268964290618896, 0.8267707228660583, 0.8271920084953308, 0.8264709711074829, 0.8273777961730957, 0.8265534043312073, 0.8263682723045349, 0.8267834186553955, 0.8264468312263489, 0.8271219730377197, 0.8266661763191223, 0.826650857925415, 0.8269800543785095, 0.8272027373313904, 0.8265773057937622, 0.8268014788627625, 0.8268194794654846, 0.8267420530319214, 0.8268665075302124, 0.8266637325286865, 0.827096700668335, 0.8264058828353882, 0.8270519971847534, 0.8266001343727112, 0.8268749713897705, 0.8271494507789612, 0.8270009756088257, 0.826937198638916, 0.8269516229629517, 0.8270692229270935, 0.8266440033912659, 0.8270074725151062, 0.8270453810691833, 0.8272398710250854, 0.8266633749008179, 0.8267764449119568, 0.8272260427474976, 0.8268193006515503, 0.8267767429351807, 0.8268207311630249, 0.827318012714386, 0.8265374302864075, 0.8270682692527771, 0.8268754482269287, 0.8268120884895325, 0.8267786502838135, 0.8268232941627502, 0.8270293474197388, 0.8269273042678833, 0.82720547914505, 0.8269999623298645, 0.8269137144088745, 0.8269937634468079, 0.8261711001396179, 0.827258288860321, 0.8266165256500244, 0.8267888426780701, 0.8269332647323608, 0.8271878957748413, 0.8265584707260132, 0.8267943263053894, 0.827041506767273, 0.8266675472259521, 0.8268055319786072, 0.8266867399215698, 0.8267763257026672, 0.8265514373779297, 0.8267965912818909, 0.8269230723381042, 0.8268799781799316, 0.8268845677375793, 0.8268047571182251, 0.8268822431564331, 0.8265044093132019, 0.8268688917160034, 0.8264381289482117, 0.8275502920150757, 0.8271793723106384, 0.8267961740493774, 0.8276157379150391, 0.8270190358161926, 0.8266755938529968, 0.8270390033721924, 0.8267343640327454, 0.8269718885421753, 0.8271317481994629, 0.8268503546714783, 0.8266642093658447, 0.8267942667007446, 0.8267762064933777, 0.8268392086029053, 0.8267672061920166, 0.8270443081855774, 0.8269408345222473, 0.8272304534912109, 0.8268920183181763, 0.8268342018127441, 0.8267329931259155, 0.8274599313735962, 0.8264439702033997, 0.8265095949172974, 0.8270931243896484, 0.8265930414199829, 0.8263660669326782, 0.826958417892456, 0.827071487903595, 0.8271905779838562, 0.8265401124954224, 0.8264755606651306, 0.8271111249923706, 0.8267589211463928, 0.8270600438117981, 0.8268613815307617, 0.8269447088241577, 0.8271568417549133, 0.8266616463661194, 0.8265151381492615, 0.8266466856002808, 0.8269634246826172, 0.8268913626670837, 0.8267860412597656, 0.8270264863967896, 0.8266427516937256, 0.8269829154014587, 0.8270019888877869, 0.8273637890815735, 0.8268808722496033, 0.8272119760513306, 0.8267934918403625, 0.8270646929740906, 0.8269793391227722, 0.8269771933555603, 0.8263921141624451, 0.8268453478813171, 0.8268986940383911, 0.8268758654594421, 0.8269860148429871, 0.8269736766815186, 0.8263589143753052, 0.8270196318626404, 0.8271034955978394, 0.8267056941986084, 0.8269591927528381, 0.8267936706542969, 0.8268769383430481, 0.8267797827720642, 0.826812207698822, 0.8268753290176392, 0.8266189694404602, 0.8270620107650757, 0.8268044590950012, 0.8270045518875122, 0.8273154497146606, 0.8266966938972473, 0.8265207409858704], 'poisson': [0.8681961894035339, 0.8277866840362549, 0.8266697525978088, 0.8271215558052063, 0.8265001773834229, 0.8269814252853394, 0.8267329335212708, 0.8268933892250061, 0.8270233869552612, 0.8266149163246155, 0.827154815196991, 0.8271946907043457, 0.826579749584198, 0.8268014192581177, 0.8268925547599792, 0.8267494440078735, 0.8270748257637024, 0.826631486415863, 0.8267567753791809, 0.8267937302589417, 0.8268844485282898, 0.8266299366950989, 0.8267364501953125, 0.8270810842514038, 0.8266992568969727, 0.8268337249755859, 0.8268970847129822, 0.8269334435462952, 0.8265900015830994, 0.826556921005249, 0.8270639777183533, 0.8270509243011475, 0.8265610933303833, 0.8267791867256165, 0.826957106590271, 0.8270310759544373, 0.8265109062194824, 0.8269197344779968, 0.8266429305076599, 0.8265719413757324, 0.8265510201454163, 0.8269957304000854, 0.8269032835960388, 0.8269296884536743, 0.8271241784095764, 0.8266963362693787, 0.8276617527008057, 0.826846718788147, 0.826876163482666, 0.8271520137786865, 0.8267638683319092, 0.8266553282737732, 0.8273570537567139, 0.8271679878234863, 0.826922595500946, 0.8268201351165771, 0.8270400166511536, 0.8270386457443237, 0.8268786072731018, 0.8270329236984253, 0.8266760110855103, 0.8269174695014954, 0.8270733952522278, 0.8263143301010132, 0.8271083831787109, 0.8267224431037903, 0.8267468810081482, 0.8266358971595764, 0.8266815543174744, 0.8266082406044006, 0.8270987868309021, 0.8266881704330444, 0.8269663453102112, 0.8267425894737244, 0.8270224332809448, 0.8269215822219849, 0.827211320400238, 0.82683265209198, 0.8267722129821777, 0.8270087838172913, 0.8270189166069031, 0.8266458511352539, 0.8269816637039185, 0.826474666595459, 0.8264777064323425, 0.8271064162254333, 0.8269944787025452, 0.826701283454895, 0.827031135559082, 0.8270744681358337, 0.8271805047988892, 0.8266415596008301, 0.8267378807067871, 0.8269835710525513, 0.8268670439720154, 0.8270387053489685, 0.8268299698829651, 0.827354907989502, 0.8267204761505127, 0.8265712261199951, 0.8265804052352905, 0.827356219291687, 0.8270542025566101, 0.826755940914154, 0.8269258737564087, 0.8263602256774902, 0.8268926739692688, 0.8269543051719666, 0.8269262909889221, 0.8266429305076599, 0.8267120718955994, 0.8267315030097961, 0.8268006443977356, 0.8268387913703918, 0.8267667293548584, 0.8270542621612549, 0.8268669247627258, 0.8269286751747131, 0.8269023299217224, 0.8270446062088013, 0.8266289234161377, 0.8266321420669556, 0.8271960020065308, 0.8269605040550232, 0.8266164660453796, 0.826697051525116, 0.8268269300460815, 0.8266709446907043, 0.8266257047653198, 0.8271938562393188, 0.8265153765678406, 0.8268263936042786, 0.8268808722496033, 0.8274499773979187, 0.8264982104301453, 0.8268078565597534, 0.8269666433334351, 0.8263806700706482, 0.8271584510803223, 0.826622486114502, 0.8266633749008179, 0.8272151947021484, 0.8269597291946411, 0.8269526958465576, 0.8268177509307861, 0.8268964290618896, 0.8267707228660583, 0.8271920084953308, 0.8264709711074829, 0.8273777961730957, 0.8265534043312073, 0.8263682723045349, 0.8267834186553955, 0.8264468312263489, 0.8271219730377197, 0.8266661763191223, 0.826650857925415, 0.8269800543785095, 0.8272027373313904, 0.8265773057937622, 0.8268014788627625, 0.8268194794654846, 0.8267420530319214, 0.8268665075302124, 0.8266637325286865, 0.827096700668335, 0.8264058828353882, 0.8270519971847534, 0.8266001343727112, 0.8268749713897705, 0.8271494507789612, 0.8270009756088257, 0.826937198638916, 0.8269516229629517, 0.8270692229270935, 0.8266440033912659, 0.8270074725151062, 0.8270453810691833, 0.8272398710250854, 0.8266633749008179, 0.8267764449119568, 0.8272260427474976, 0.8268193006515503, 0.8267767429351807, 0.8268207311630249, 0.827318012714386, 0.8265374302864075, 0.8270682692527771, 0.8268754482269287, 0.8268120884895325, 0.8267786502838135, 0.8268232941627502, 0.8270293474197388, 0.8269273042678833, 0.82720547914505, 0.8269999623298645, 0.8269137144088745, 0.8269937634468079, 0.8261711001396179, 0.827258288860321, 0.8266165256500244, 0.8267888426780701, 0.8269332647323608, 0.8271878957748413, 0.8265584707260132, 0.8267943263053894, 0.827041506767273, 0.8266675472259521, 0.8268055319786072, 0.8266867399215698, 0.8267763257026672, 0.8265514373779297, 0.8267965912818909, 0.8269230723381042, 0.8268799781799316, 0.8268845677375793, 0.8268047571182251, 0.8268822431564331, 0.8265044093132019, 0.8268688917160034, 0.8264381289482117, 0.8275502920150757, 0.8271793723106384, 0.8267961740493774, 0.8276157379150391, 0.8270190358161926, 0.8266755938529968, 0.8270390033721924, 0.8267343640327454, 0.8269718885421753, 0.8271317481994629, 0.8268503546714783, 0.8266642093658447, 0.8267942667007446, 0.8267762064933777, 0.8268392086029053, 0.8267672061920166, 0.8270443081855774, 0.8269408345222473, 0.8272304534912109, 0.8268920183181763, 0.8268342018127441, 0.8267329931259155, 0.8274599313735962, 0.8264439702033997, 0.8265095949172974, 0.8270931243896484, 0.8265930414199829, 0.8263660669326782, 0.826958417892456, 0.827071487903595, 0.8271905779838562, 0.8265401124954224, 0.8264755606651306, 0.8271111249923706, 0.8267589211463928, 0.8270600438117981, 0.8268613815307617, 0.8269447088241577, 0.8271568417549133, 0.8266616463661194, 0.8265151381492615, 0.8266466856002808, 0.8269634246826172, 0.8268913626670837, 0.8267860412597656, 0.8270264863967896, 0.8266427516937256, 0.8269829154014587, 0.8270019888877869, 0.8273637890815735, 0.8268808722496033, 0.8272119760513306, 0.8267934918403625, 0.8270646929740906, 0.8269793391227722, 0.8269771933555603, 0.8263921141624451, 0.8268453478813171, 0.8268986940383911, 0.8268758654594421, 0.8269860148429871, 0.8269736766815186, 0.8263589143753052, 0.8270196318626404, 0.8271034955978394, 0.8267056941986084, 0.8269591927528381, 0.8267936706542969, 0.8268769383430481, 0.8267797827720642, 0.826812207698822, 0.8268753290176392, 0.8266189694404602, 0.8270620107650757, 0.8268044590950012, 0.8270045518875122, 0.8273154497146606, 0.8266966938972473, 0.8265207409858704], 'val_loss': [0.8296114206314087, 0.8280065059661865, 0.8380330204963684, 0.8261351585388184, 0.8260001540184021, 0.8261792659759521, 0.8271605372428894, 0.8268503546714783, 0.8392996191978455, 0.8276119232177734, 0.8270278573036194, 0.8281062245368958, 0.8262368440628052, 0.8295310139656067, 0.8267245888710022, 0.8269084692001343, 0.8264173269271851, 0.8317381739616394, 0.8260618448257446, 0.8330270051956177, 0.8258477449417114, 0.8275355696678162, 0.8302373290061951, 0.833864688873291, 0.8265019655227661, 0.8289759159088135, 0.826241135597229, 0.8261796236038208, 0.8259624242782593, 0.8268955945968628, 0.8258621692657471, 0.8289543986320496, 0.8308449983596802, 0.825881838798523, 0.830818772315979, 0.8260234594345093, 0.8291565775871277, 0.8336308598518372, 0.8258106112480164, 0.8326157927513123, 0.8270383477210999, 0.8261792063713074, 0.8266615271568298, 0.8319015502929688, 0.8264287114143372, 0.8258894085884094, 0.8256581425666809, 0.8276658654212952, 0.8277725577354431, 0.8271337151527405, 0.8311581015586853, 0.8260433077812195, 0.8323589563369751, 0.8269666433334351, 0.8265398144721985, 0.8301765322685242, 0.8315953612327576, 0.8285741209983826, 0.8299522399902344, 0.8428558707237244, 0.8315754532814026, 0.8283510208129883, 0.8266806602478027, 0.8276554942131042, 0.8265547156333923, 0.8283249735832214, 0.8260343670845032, 0.8264864683151245, 0.8301600217819214, 0.8275558948516846, 0.8255655765533447, 0.8280162811279297, 0.8264520764350891, 0.8273070454597473, 0.8264941573143005, 0.8258156776428223, 0.8262038826942444, 0.82623690366745, 0.8280001878738403, 0.8265010714530945, 0.826955258846283, 0.8287847638130188, 0.8261619210243225, 0.826084315776825, 0.828534722328186, 0.8358812928199768, 0.8313159346580505, 0.8302315473556519, 0.8269638419151306, 0.8260691165924072, 0.826938807964325, 0.8313781023025513, 0.828617513179779, 0.8299763798713684, 0.8287172913551331, 0.8262389302253723, 0.8323989510536194, 0.8325164914131165, 0.8342715501785278, 0.8280105590820312, 0.8359555006027222, 0.8345361351966858, 0.8288996815681458, 0.8269881010055542, 0.8265565633773804, 0.8274012207984924, 0.8265450596809387, 0.8272863030433655, 0.8266214728355408, 0.8345877528190613, 0.8295791149139404, 0.8271687626838684, 0.8282408118247986, 0.8259246349334717, 0.8269651532173157, 0.8268203139305115, 0.8265007138252258, 0.8266246914863586, 0.8283739686012268, 0.8265545964241028, 0.8260538578033447, 0.8324684500694275, 0.8262117505073547, 0.8263803720474243, 0.8262128829956055, 0.8271797895431519, 0.8282343745231628, 0.8271971940994263, 0.8374303579330444, 0.8265812993049622, 0.8277634978294373, 0.8273301720619202, 0.8342487812042236, 0.8274944424629211, 0.8263261914253235, 0.826921820640564, 0.8268879652023315, 0.8280431032180786, 0.8306059837341309, 0.8260440826416016, 0.8288772106170654, 0.8271595239639282, 0.8267914056777954, 0.8263338208198547, 0.8257560133934021, 0.8273007273674011, 0.8282167911529541, 0.8274704813957214, 0.8263192772865295, 0.8269286155700684, 0.8310288190841675, 0.826632559299469, 0.828612208366394, 0.8348199725151062, 0.8347676396369934, 0.8322613835334778, 0.8297366499900818, 0.8285825252532959, 0.8324388265609741, 0.8314778804779053, 0.827034592628479, 0.8298079967498779, 0.8270628452301025, 0.8266271948814392, 0.8283185362815857, 0.8262090086936951, 0.833821713924408, 0.8303160667419434, 0.8261988759040833, 0.8264278173446655, 0.8365925550460815, 0.8474882245063782, 0.8396183252334595, 0.826908528804779, 0.8269893527030945, 0.8257805705070496, 0.8273723125457764, 0.8259785771369934, 0.8263718485832214, 0.8303272128105164, 0.8370080590248108, 0.8288800716400146, 0.8264888525009155, 0.8265021443367004, 0.8261730074882507, 0.8300507664680481, 0.8289284110069275, 0.827548086643219, 0.8281862735748291, 0.8266595005989075, 0.8259615898132324, 0.8264002799987793, 0.8266145586967468, 0.8305717706680298, 0.826381266117096, 0.8315716981887817, 0.8262265920639038, 0.8262993097305298, 0.8288875222206116, 0.8263165950775146, 0.8336675763130188, 0.8261181712150574, 0.8261749744415283, 0.8318769335746765, 0.8281022906303406, 0.8267267942428589, 0.8307009339332581, 0.8272621035575867, 0.8260703086853027, 0.831732988357544, 0.8284293413162231, 0.826128363609314, 0.8266939520835876, 0.8279840350151062, 0.8271760940551758, 0.8323488235473633, 0.8261809945106506, 0.8288939595222473, 0.8260940909385681, 0.8270538449287415, 0.8264752626419067, 0.826903223991394, 0.8264363408088684, 0.8272020816802979, 0.8264193534851074, 0.8286620378494263, 0.8272314667701721, 0.828508198261261, 0.8296599984169006, 0.8281430006027222, 0.8267949223518372, 0.8262596726417542, 0.8309038281440735, 0.8276290893554688, 0.8281933665275574, 0.8282109498977661, 0.8260979652404785, 0.8294363617897034, 0.8276688456535339, 0.8261182308197021, 0.8289536833763123, 0.825987696647644, 0.826082170009613, 0.8259934782981873, 0.8265030980110168, 0.8297373056411743, 0.826836347579956, 0.8282538056373596, 0.826270580291748, 0.8261825442314148, 0.8271622061729431, 0.8399274945259094, 0.8266975283622742, 0.8261963129043579, 0.834330141544342, 0.8350743055343628, 0.8257105350494385, 0.8279260396957397, 0.8258549571037292, 0.8285577893257141, 0.8261237144470215, 0.8263590335845947, 0.8263604044914246, 0.8263531923294067, 0.8359227776527405, 0.837846040725708, 0.8280134797096252, 0.8275921940803528, 0.8260375261306763, 0.8259549140930176, 0.8262999057769775, 0.8306845426559448, 0.8273674845695496, 0.833210289478302, 0.8282949328422546, 0.8260658979415894, 0.8260066509246826, 0.8282869458198547, 0.8277302980422974, 0.8278300166130066, 0.8266137838363647, 0.8298212289810181, 0.8271958827972412, 0.8263453841209412, 0.8273931741714478, 0.8312336206436157, 0.8274585604667664, 0.8292782306671143, 0.8310860395431519, 0.8264786601066589, 0.8301036953926086, 0.8328919410705566, 0.8284052610397339, 0.827760636806488, 0.8273872137069702, 0.832279622554779, 0.8307633399963379, 0.8307866454124451, 0.8292422890663147, 0.8297230005264282], 'val_poisson': [0.8296114206314087, 0.8280065059661865, 0.8380330204963684, 0.8261351585388184, 0.8260001540184021, 0.8261792659759521, 0.8271605372428894, 0.8268503546714783, 0.8392996191978455, 0.8276119232177734, 0.8270278573036194, 0.8281062245368958, 0.8262368440628052, 0.8295310139656067, 0.8267245888710022, 0.8269084692001343, 0.8264173269271851, 0.8317381739616394, 0.8260618448257446, 0.8330270051956177, 0.8258477449417114, 0.8275355696678162, 0.8302373290061951, 0.833864688873291, 0.8265019655227661, 0.8289759159088135, 0.826241135597229, 0.8261796236038208, 0.8259624242782593, 0.8268955945968628, 0.8258621692657471, 0.8289543986320496, 0.8308449983596802, 0.825881838798523, 0.830818772315979, 0.8260234594345093, 0.8291565775871277, 0.8336308598518372, 0.8258106112480164, 0.8326157927513123, 0.8270383477210999, 0.8261792063713074, 0.8266615271568298, 0.8319015502929688, 0.8264287114143372, 0.8258894085884094, 0.8256581425666809, 0.8276658654212952, 0.8277725577354431, 0.8271337151527405, 0.8311581015586853, 0.8260433077812195, 0.8323589563369751, 0.8269666433334351, 0.8265398144721985, 0.8301765322685242, 0.8315953612327576, 0.8285741209983826, 0.8299522399902344, 0.8428558707237244, 0.8315754532814026, 0.8283510208129883, 0.8266806602478027, 0.8276554942131042, 0.8265547156333923, 0.8283249735832214, 0.8260343670845032, 0.8264864683151245, 0.8301600217819214, 0.8275558948516846, 0.8255655765533447, 0.8280162811279297, 0.8264520764350891, 0.8273070454597473, 0.8264941573143005, 0.8258156776428223, 0.8262038826942444, 0.82623690366745, 0.8280001878738403, 0.8265010714530945, 0.826955258846283, 0.8287847638130188, 0.8261619210243225, 0.826084315776825, 0.828534722328186, 0.8358812928199768, 0.8313159346580505, 0.8302315473556519, 0.8269638419151306, 0.8260691165924072, 0.826938807964325, 0.8313781023025513, 0.828617513179779, 0.8299763798713684, 0.8287172913551331, 0.8262389302253723, 0.8323989510536194, 0.8325164914131165, 0.8342715501785278, 0.8280105590820312, 0.8359555006027222, 0.8345361351966858, 0.8288996815681458, 0.8269881010055542, 0.8265565633773804, 0.8274012207984924, 0.8265450596809387, 0.8272863030433655, 0.8266214728355408, 0.8345877528190613, 0.8295791149139404, 0.8271687626838684, 0.8282408118247986, 0.8259246349334717, 0.8269651532173157, 0.8268203139305115, 0.8265007138252258, 0.8266246914863586, 0.8283739686012268, 0.8265545964241028, 0.8260538578033447, 0.8324684500694275, 0.8262117505073547, 0.8263803720474243, 0.8262128829956055, 0.8271797895431519, 0.8282343745231628, 0.8271971940994263, 0.8374303579330444, 0.8265812993049622, 0.8277634978294373, 0.8273301720619202, 0.8342487812042236, 0.8274944424629211, 0.8263261914253235, 0.826921820640564, 0.8268879652023315, 0.8280431032180786, 0.8306059837341309, 0.8260440826416016, 0.8288772106170654, 0.8271595239639282, 0.8267914056777954, 0.8263338208198547, 0.8257560133934021, 0.8273007273674011, 0.8282167911529541, 0.8274704813957214, 0.8263192772865295, 0.8269286155700684, 0.8310288190841675, 0.826632559299469, 0.828612208366394, 0.8348199725151062, 0.8347676396369934, 0.8322613835334778, 0.8297366499900818, 0.8285825252532959, 0.8324388265609741, 0.8314778804779053, 0.827034592628479, 0.8298079967498779, 0.8270628452301025, 0.8266271948814392, 0.8283185362815857, 0.8262090086936951, 0.833821713924408, 0.8303160667419434, 0.8261988759040833, 0.8264278173446655, 0.8365925550460815, 0.8474882245063782, 0.8396183252334595, 0.826908528804779, 0.8269893527030945, 0.8257805705070496, 0.8273723125457764, 0.8259785771369934, 0.8263718485832214, 0.8303272128105164, 0.8370080590248108, 0.8288800716400146, 0.8264888525009155, 0.8265021443367004, 0.8261730074882507, 0.8300507664680481, 0.8289284110069275, 0.827548086643219, 0.8281862735748291, 0.8266595005989075, 0.8259615898132324, 0.8264002799987793, 0.8266145586967468, 0.8305717706680298, 0.826381266117096, 0.8315716981887817, 0.8262265920639038, 0.8262993097305298, 0.8288875222206116, 0.8263165950775146, 0.8336675763130188, 0.8261181712150574, 0.8261749744415283, 0.8318769335746765, 0.8281022906303406, 0.8267267942428589, 0.8307009339332581, 0.8272621035575867, 0.8260703086853027, 0.831732988357544, 0.8284293413162231, 0.826128363609314, 0.8266939520835876, 0.8279840350151062, 0.8271760940551758, 0.8323488235473633, 0.8261809945106506, 0.8288939595222473, 0.8260940909385681, 0.8270538449287415, 0.8264752626419067, 0.826903223991394, 0.8264363408088684, 0.8272020816802979, 0.8264193534851074, 0.8286620378494263, 0.8272314667701721, 0.828508198261261, 0.8296599984169006, 0.8281430006027222, 0.8267949223518372, 0.8262596726417542, 0.8309038281440735, 0.8276290893554688, 0.8281933665275574, 0.8282109498977661, 0.8260979652404785, 0.8294363617897034, 0.8276688456535339, 0.8261182308197021, 0.8289536833763123, 0.825987696647644, 0.826082170009613, 0.8259934782981873, 0.8265030980110168, 0.8297373056411743, 0.826836347579956, 0.8282538056373596, 0.826270580291748, 0.8261825442314148, 0.8271622061729431, 0.8399274945259094, 0.8266975283622742, 0.8261963129043579, 0.834330141544342, 0.8350743055343628, 0.8257105350494385, 0.8279260396957397, 0.8258549571037292, 0.8285577893257141, 0.8261237144470215, 0.8263590335845947, 0.8263604044914246, 0.8263531923294067, 0.8359227776527405, 0.837846040725708, 0.8280134797096252, 0.8275921940803528, 0.8260375261306763, 0.8259549140930176, 0.8262999057769775, 0.8306845426559448, 0.8273674845695496, 0.833210289478302, 0.8282949328422546, 0.8260658979415894, 0.8260066509246826, 0.8282869458198547, 0.8277302980422974, 0.8278300166130066, 0.8266137838363647, 0.8298212289810181, 0.8271958827972412, 0.8263453841209412, 0.8273931741714478, 0.8312336206436157, 0.8274585604667664, 0.8292782306671143, 0.8310860395431519, 0.8264786601066589, 0.8301036953926086, 0.8328919410705566, 0.8284052610397339, 0.827760636806488, 0.8273872137069702, 0.832279622554779, 0.8307633399963379, 0.8307866454124451, 0.8292422890663147, 0.8297230005264282]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01589823],\n",
      "       [ 0.02660634],\n",
      "       [-0.00374579],\n",
      "       [ 0.00974168],\n",
      "       [ 0.02316013],\n",
      "       [ 0.00779289],\n",
      "       [ 0.00986268],\n",
      "       [ 0.01482649],\n",
      "       [ 0.00613408],\n",
      "       [ 0.02921264],\n",
      "       [ 0.02349446],\n",
      "       [ 0.01559466],\n",
      "       [ 0.03024372],\n",
      "       [ 0.00095859],\n",
      "       [ 0.00207945],\n",
      "       [ 0.00498902],\n",
      "       [ 0.01995845],\n",
      "       [ 0.00740979],\n",
      "       [ 0.0105727 ],\n",
      "       [ 0.01595516],\n",
      "       [ 0.02921729],\n",
      "       [ 0.02490989],\n",
      "       [ 0.00467117],\n",
      "       [ 0.02409012],\n",
      "       [ 0.02050732],\n",
      "       [ 0.01469798],\n",
      "       [-0.00951539],\n",
      "       [ 0.02237457],\n",
      "       [ 0.0294974 ],\n",
      "       [ 0.01599086],\n",
      "       [ 0.00967024],\n",
      "       [ 0.0104715 ],\n",
      "       [ 0.0286387 ],\n",
      "       [-0.00083455],\n",
      "       [ 0.02491707],\n",
      "       [ 0.02577025],\n",
      "       [ 0.01350615],\n",
      "       [-0.00067979],\n",
      "       [ 0.02710109],\n",
      "       [-0.00155706],\n",
      "       [ 0.01449263],\n",
      "       [ 0.01367791],\n",
      "       [-0.00544295],\n",
      "       [ 0.012048  ],\n",
      "       [ 0.00832329],\n",
      "       [-0.00298752],\n",
      "       [ 0.01392381],\n",
      "       [-0.00771896],\n",
      "       [ 0.01419691],\n",
      "       [ 0.01044945],\n",
      "       [ 0.00297289],\n",
      "       [-0.01006117],\n",
      "       [ 0.00210444],\n",
      "       [-0.00725254],\n",
      "       [ 0.00844307],\n",
      "       [ 0.03789328],\n",
      "       [ 0.00983779],\n",
      "       [ 0.03215289],\n",
      "       [ 0.01549408],\n",
      "       [ 0.01089572],\n",
      "       [ 0.01060283],\n",
      "       [ 0.02102689],\n",
      "       [ 0.02664259],\n",
      "       [ 0.02920857],\n",
      "       [ 0.00729109],\n",
      "       [ 0.00354606],\n",
      "       [ 0.00273187],\n",
      "       [ 0.01342   ],\n",
      "       [ 0.01700279],\n",
      "       [ 0.00375428],\n",
      "       [ 0.01178233],\n",
      "       [-0.000417  ],\n",
      "       [ 0.00029308],\n",
      "       [ 0.01207929],\n",
      "       [-0.00440426],\n",
      "       [ 0.01065359],\n",
      "       [-0.00068445],\n",
      "       [ 0.00975986],\n",
      "       [ 0.02322699],\n",
      "       [ 0.02046507],\n",
      "       [ 0.01659164],\n",
      "       [-0.00767765],\n",
      "       [-0.0125524 ],\n",
      "       [ 0.01626215],\n",
      "       [ 0.00477136],\n",
      "       [-0.01972607],\n",
      "       [ 0.01465809],\n",
      "       [ 0.01635291],\n",
      "       [ 0.00648135],\n",
      "       [-0.01055053],\n",
      "       [ 0.01894802],\n",
      "       [ 0.00999258],\n",
      "       [-0.01508743],\n",
      "       [ 0.00994277],\n",
      "       [ 0.02378165],\n",
      "       [ 0.01916969],\n",
      "       [ 0.02110635],\n",
      "       [ 0.02926962],\n",
      "       [ 0.01224158],\n",
      "       [ 0.01419146]], dtype=float32), array([-0.00808195], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Keras training pregress.\n",
    "loss and Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Keras training progress');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data using `evaluate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "625/625 - 1s - loss: 0.8228 - poisson: 0.8228 - 503ms/epoch - 804us/step\n",
      "test loss, test acc: [0.8228042721748352, 0.8228042721748352]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the model on the test data using `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 908us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predictions and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a reload like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a keras model\n",
    "model = keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
