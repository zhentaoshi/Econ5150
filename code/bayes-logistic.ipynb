{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07917f7d",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression Simulation\n",
    "\n",
    "This notebook simulates binary data, fits Bayesian logistic regression, and summarizes posterior inference for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38f0b8",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "For $i=1,\\ldots,n$, let $y_i\\in\\{0,1\\}$ and $x_i\\in\\mathbb{R}^p$.\n",
    "\n",
    "$$\n",
    "\\Pr(y_i=1\\mid x_i,\\theta)=\\sigma(x_i'\\theta),\\qquad \\sigma(t)=\\frac{1}{1+e^{-t}}.\n",
    "$$\n",
    "\n",
    "The likelihood is\n",
    "$$\n",
    "p(y\\mid\\theta)=\\prod_{i=1}^n \\sigma(x_i'\\theta)^{y_i}\\left[1-\\sigma(x_i'\\theta)\\right]^{1-y_i}.\n",
    "$$\n",
    "\n",
    "We use a Gaussian prior:\n",
    "$$\n",
    "\\theta\\sim N(0,\\tau^2I_p).\n",
    "$$\n",
    "\n",
    "The posterior is proportional to\n",
    "$$\n",
    "p(\\theta\\mid y)\\propto p(y\\mid\\theta)\\,p(\\theta).\n",
    "$$\n",
    "\n",
    "Sampling is done with `emcee` using a Gaussian Metropolis-Hastings proposal (`emcee.moves.GaussianMove`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32dc3574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:33.129646Z",
     "iopub.status.busy": "2026-02-23T14:00:33.129646Z",
     "iopub.status.idle": "2026-02-23T14:00:41.430878Z",
     "shell.execute_reply": "2026-02-23T14:00:41.430878Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0e505c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:41.433386Z",
     "iopub.status.busy": "2026-02-23T14:00:41.433386Z",
     "iopub.status.idle": "2026-02-23T14:00:41.439852Z",
     "shell.execute_reply": "2026-02-23T14:00:41.439852Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_logistic_data(n_obs, beta_true, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x1 = rng.normal(size=n_obs)\n",
    "    x2 = rng.normal(size=n_obs)\n",
    "    X = np.column_stack([np.ones(n_obs), x1, x2])\n",
    "    eta = X @ beta_true\n",
    "    p = 1.0 / (1.0 + np.exp(-eta))\n",
    "    y = rng.binomial(1, p, size=n_obs)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def log_prior(theta, prior_sd):\n",
    "    if not np.all(np.isfinite(theta)):\n",
    "        return -np.inf\n",
    "    return -0.5 * np.sum((theta / prior_sd) ** 2)\n",
    "\n",
    "\n",
    "def log_likelihood(theta, X, y):\n",
    "    eta = X @ theta\n",
    "    return np.sum(y * eta - np.logaddexp(0.0, eta))\n",
    "\n",
    "\n",
    "def log_posterior(theta, X, y, prior_sd):\n",
    "    lp = log_prior(theta, prior_sd)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, X, y)\n",
    "\n",
    "\n",
    "def run_mcmc(X, y, prior_sd, seed, n_walkers, burn_in, n_steps, thin, proposal_scale):\n",
    "    rng = np.random.default_rng(seed + 1)\n",
    "    n_dim = X.shape[1]\n",
    "    if n_walkers < 2 * n_dim:\n",
    "        raise ValueError(f\"n_walkers must be at least {2 * n_dim}.\")\n",
    "\n",
    "    initial = rng.normal(0.0, 0.2, size=(n_walkers, n_dim))\n",
    "    mh_move = emcee.moves.GaussianMove(cov=(proposal_scale**2) * np.eye(n_dim), mode=\"vector\")\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers=n_walkers,\n",
    "        ndim=n_dim,\n",
    "        log_prob_fn=log_posterior,\n",
    "        args=(X, y, prior_sd),\n",
    "        moves=mh_move,\n",
    "    )\n",
    "\n",
    "    state = sampler.run_mcmc(initial, burn_in, progress=False)\n",
    "    sampler.reset()\n",
    "    sampler.run_mcmc(state, n_steps, progress=False)\n",
    "    samples = sampler.get_chain(flat=True, thin=thin)\n",
    "    accept_rate = float(np.mean(sampler.acceptance_fraction))\n",
    "    return samples, accept_rate\n",
    "\n",
    "\n",
    "def summarize(samples):\n",
    "    post_mean = np.mean(samples, axis=0)\n",
    "    ci_low = np.quantile(samples, 0.025, axis=0)\n",
    "    ci_high = np.quantile(samples, 0.975, axis=0)\n",
    "    return post_mean, ci_low, ci_high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6e9cd",
   "metadata": {},
   "source": [
    "## Simulation Design\n",
    "\n",
    "True parameter: $\\theta_0 = (-0.4,\\;1.1,\\;-1.6)'$.\n",
    "\n",
    "MCMC settings below are chosen to run quickly while still giving stable posterior summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea23706b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:41.439852Z",
     "iopub.status.busy": "2026-02-23T14:00:41.439852Z",
     "iopub.status.idle": "2026-02-23T14:00:41.446380Z",
     "shell.execute_reply": "2026-02-23T14:00:41.446380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs=1000, mean(y)=0.458\n"
     ]
    }
   ],
   "source": [
    "seed = 2026\n",
    "n_obs = 1000\n",
    "beta_true = np.array([-0.4, 1.1, -1.6])\n",
    "prior_sd = 2.5\n",
    "n_walkers = 40\n",
    "burn_in = 1200\n",
    "n_steps = 2200\n",
    "thin = 5\n",
    "proposal_scale = 0.08\n",
    "\n",
    "X, y = simulate_logistic_data(n_obs=n_obs, beta_true=beta_true, seed=seed)\n",
    "print(f\"n_obs={n_obs}, mean(y)={y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404625f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:41.449172Z",
     "iopub.status.busy": "2026-02-23T14:00:41.449172Z",
     "iopub.status.idle": "2026-02-23T14:00:46.061921Z",
     "shell.execute_reply": "2026-02-23T14:00:46.061921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean acceptance fraction: 0.493\n",
      "name      true      post_mean    2.5%       97.5%\n",
      "intercept   -0.400      -0.432    -0.589    -0.271\n",
      "beta_1       1.100       1.023     0.846     1.211\n",
      "beta_2      -1.600      -1.632    -1.856    -1.419\n"
     ]
    }
   ],
   "source": [
    "samples, accept_rate = run_mcmc(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    prior_sd=prior_sd,\n",
    "    seed=seed,\n",
    "    n_walkers=n_walkers,\n",
    "    burn_in=burn_in,\n",
    "    n_steps=n_steps,\n",
    "    thin=thin,\n",
    "    proposal_scale=proposal_scale,\n",
    ")\n",
    "\n",
    "post_mean, ci_low, ci_high = summarize(samples)\n",
    "names = [\"intercept\", \"beta_1\", \"beta_2\"]\n",
    "\n",
    "print(f\"Mean acceptance fraction: {accept_rate:.3f}\")\n",
    "print(\"name      true      post_mean    2.5%       97.5%\")\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"{name:9s} {beta_true[i]:8.3f} {post_mean[i]:11.3f} {ci_low[i]:9.3f} {ci_high[i]:9.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8f2a1",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "MCMC targets the exact posterior (up to Monte Carlo error), but it can be slow at scale. Variational inference (VI) replaces sampling with an optimization problem: choose a tractable family $q(\\theta)$ and fit it to approximate $p(\\theta\\mid y)$.\n",
    "\n",
    "For Bayesian logistic regression with a Gaussian prior $p(\\theta)=N(0,\\tau^2 I)$, a common VI approximation is Gaussian, $q(\\theta)=N(m,S)$. Using the Jaakkola-Jordan quadratic bound for the logistic likelihood yields coordinate updates for $(m,S)$ and auxiliary parameters $\\{\\xi_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0b1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:46.064824Z",
     "iopub.status.busy": "2026-02-23T14:00:46.063811Z",
     "iopub.status.idle": "2026-02-23T14:00:46.075798Z",
     "shell.execute_reply": "2026-02-23T14:00:46.075798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI posterior approximation: q(theta)=N(m,S)\n",
      "name      true      vi_mean     vi_2.5%    vi_97.5%    mcmc_mean\n",
      "intercept   -0.400     -0.433     -0.570     -0.295     -0.432\n",
      "beta_1       1.100      1.031      0.892      1.170      1.023\n",
      "beta_2      -1.600     -1.633     -1.787     -1.479     -1.632\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(t):\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n",
    "\n",
    "\n",
    "def vi_logistic_jaakkola(X, y, prior_sd, max_iter=200, tol=1e-6):\n",
    "    \"\"\"Gaussian VI for Bayesian logistic regression via Jaakkola-Jordan bound.\n",
    "\n",
    "    Returns:\n",
    "        m: variational mean (p,)\n",
    "        S: variational covariance (p,p)\n",
    "        xi: local variational parameters (n,)\n",
    "    \"\"\"\n",
    "    n_obs, n_dim = X.shape\n",
    "    prior_prec = 1.0 / (prior_sd**2)\n",
    "\n",
    "    # Initialize local parameters (avoid zeros).\n",
    "    xi = np.ones(n_obs)\n",
    "    m = np.zeros(n_dim)\n",
    "    S = np.eye(n_dim) / prior_prec\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        xi_safe = np.maximum(xi, 1e-8)\n",
    "        lam = np.tanh(xi_safe / 2.0) / (4.0 * xi_safe)  # lambda(xi)\n",
    "        W = 2.0 * lam  # n_obs vector\n",
    "\n",
    "        S_inv = prior_prec * np.eye(n_dim) + X.T @ (W[:, None] * X)\n",
    "        S_new = np.linalg.inv(S_inv)\n",
    "        m_new = S_new @ (X.T @ (y - 0.5)) # an OLS type update formula\n",
    "        # this comes from an optimization step\n",
    "        # it has closed form solution due to the Gaussian specification \n",
    "        # of the family for approximation\n",
    "\n",
    "        A = S_new + np.outer(m_new, m_new)\n",
    "        xi_new = np.sqrt(np.sum((X @ A) * X, axis=1))\n",
    "\n",
    "        if np.max(np.abs(m_new - m)) < tol and np.max(np.abs(xi_new - xi)) < tol:\n",
    "            m, S, xi = m_new, S_new, xi_new\n",
    "            break\n",
    "\n",
    "        m, S, xi = m_new, S_new, xi_new\n",
    "\n",
    "    return m, S, xi\n",
    "\n",
    "\n",
    "m_vi, S_vi, xi_vi = vi_logistic_jaakkola(X=X, y=y, prior_sd=prior_sd)\n",
    "sd_vi = np.sqrt(np.diag(S_vi))\n",
    "ci_low_vi = m_vi - 1.96 * sd_vi # it does not simulate a distribution of posterior\n",
    "ci_high_vi = m_vi + 1.96 * sd_vi # instead, it uses the Gaussian to approximate the posterior\n",
    "\n",
    "print(\"VI posterior approximation: q(theta)=N(m,S)\")\n",
    "print(\"name      true      vi_mean     vi_2.5%    vi_97.5%    mcmc_mean\")\n",
    "for i, name in enumerate(names):\n",
    "    print(\n",
    "        f\"{name:9s} {beta_true[i]:8.3f} {m_vi[i]:10.3f} {ci_low_vi[i]:10.3f} {ci_high_vi[i]:10.3f} {post_mean[i]:10.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f81943",
   "metadata": {},
   "source": [
    "## Prediction Exercise (Posterior Predictive)\n",
    "\n",
    "Using posterior draws $\\{\\theta^{(s)}\\}_{s=1}^S$, the posterior predictive probability for a new feature vector $\\tilde x$ is\n",
    "$$\n",
    "\\Pr(\\tilde y=1\\mid \\tilde x, y) \\approx \\frac{1}{S}\\sum_{s=1}^S \\sigma(\\tilde x'\\theta^{(s)}).\n",
    "$$\n",
    "\n",
    "Below we evaluate prediction on a simulated holdout sample and report both point performance and posterior predictive intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c2d9ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T14:00:46.077318Z",
     "iopub.status.busy": "2026-02-23T14:00:46.077318Z",
     "iopub.status.idle": "2026-02-23T14:00:46.281379Z",
     "shell.execute_reply": "2026-02-23T14:00:46.280863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (threshold 0.5): 0.797"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test log loss: 0.404\n",
      "\n",
      "First 5 posterior predictive probabilities with 95% intervals:\n",
      "obs   y_test   p_mean    p_2.5%    p_97.5%\n",
      "  0        0    0.071     0.051     0.095\n",
      "  1        0    0.021     0.012     0.032\n",
      "  2        1    0.862     0.824     0.895\n",
      "  3        0    0.079     0.057     0.103\n",
      "  4        1    0.648     0.601     0.694\n"
     ]
    }
   ],
   "source": [
    "# Holdout data from the same DGP\n",
    "n_test = 300\n",
    "X_test, y_test = simulate_logistic_data(n_obs=n_test, beta_true=beta_true, seed=seed + 100)\n",
    "\n",
    "# Posterior predictive probabilities for each test observation\n",
    "eta_draws = samples @ X_test.T                      # shape: (S, n_test)\n",
    "pred_draws = 1.0 / (1.0 + np.exp(-eta_draws))      # shape: (S, n_test)\n",
    "pred_mean = pred_draws.mean(axis=0)\n",
    "pred_low = np.quantile(pred_draws, 0.025, axis=0)\n",
    "pred_high = np.quantile(pred_draws, 0.975, axis=0)\n",
    "\n",
    "# Classification metrics based on posterior mean probability\n",
    "y_hat = (pred_mean >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_hat == y_test)\n",
    "eps = 1e-12\n",
    "log_loss = -np.mean(y_test * np.log(pred_mean + eps) + (1 - y_test) * np.log(1 - pred_mean + eps))\n",
    "\n",
    "print(f\"Test accuracy (threshold 0.5): {accuracy:.3f}\")\n",
    "print(f\"Test log loss: {log_loss:.3f}\")\n",
    "print(\"\")\n",
    "print(\"First 5 posterior predictive probabilities with 95% intervals:\")\n",
    "print(\"obs   y_test   p_mean    p_2.5%    p_97.5%\")\n",
    "for i in range(5):\n",
    "    print(f\"{i:3d}   {y_test[i]:6d}   {pred_mean[i]:6.3f}   {pred_low[i]:7.3f}   {pred_high[i]:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8f35e",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "- Posterior means should be close to true values in repeated simulations.\n",
    "- The 95% credible intervals should typically include the true coefficients at this sample size.\n",
    "- The acceptance fraction can be tuned with `proposal_scale`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
