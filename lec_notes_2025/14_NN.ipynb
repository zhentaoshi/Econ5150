{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "### Zhentao Shi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The workhorse of AI\n",
    "\n",
    "## Types of Neural Networks\n",
    "\n",
    "\n",
    "* Feedforward (FNN)\n",
    "* Convolution (CNN)\n",
    "* Recurrent (RNN)\n",
    "* Long short-term memory (LSTM)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = b_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = \\sigma ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $\\sigma(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choices\n",
    "  * Sigmoid: $1/(1+\\exp(-x))$\n",
    "  * Rectified linear unit (ReLu) $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "* Feedforward: criterion evaluation\n",
    "* Back propagation: parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* One-layer feedforward NN for demonstration\n",
    "* Input: $p$\n",
    "* Hidden nodes: $K$\n",
    "  \n",
    "* Criterion: \n",
    "$$\n",
    "\\min_{\\theta}   \\frac{1}{2}\\sum_{i=1}^n  Q_i \\textrm{ where } Q_i = [y_i - \\hat{y}_i ]^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Components in Optimization\n",
    "\n",
    "* Input -> hidden layer is indexed as (1), \n",
    "* The hidden -> output layer as (2):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_i & =  b^{(2)} + \\sum_{j=1}^K w_{j}^{(2)} a_j^{(1)} \\\\ \n",
    "a_j^{(1)} & = \\sigma \\left( z^{(1)}_j\\right) \\\\\n",
    "z_j^{(1)} & =b_j^{(1)} + \\sum_{\\ell=1}^p w_{j\\ell}^{(1)} x_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Intercept is called **bias**\n",
    "* Slope coefficient is called **weight**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient method\n",
    "\n",
    "Taylor expansion\n",
    "\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) (\\theta_{t+1}-\\theta_{t}) + h.o.t.\n",
    "$$\n",
    "where\n",
    "* $\\nabla Q(\\theta_t)$ is **Gradient**\n",
    "* $(\\theta_{t+1}-\\theta_{t})$ is unknown, use $p_t$ to replace it as\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) p_t\n",
    "$$\n",
    "which direction reduces the value of function?\n",
    "\n",
    "* Choose $ \\theta_{t+1}-\\theta_{t} = p_t = - \\alpha \\cdot \\nabla Q(\\theta_t)$ ensures reduction in $Q$, where $\\alpha$ is the **learning rate**, and we obtain the iterative formula\n",
    "$$\n",
    " \\theta_{t+1} = \\theta_{t} - \\alpha \\cdot \\nabla Q(\\theta_t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Output layer -> hidden layer\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial b^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\sigma\\left(z_{j}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* Hidden layer -> input layer: \n",
    "  * NN is a composite function. By the chain rule \n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial b^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial b^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)x_{i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: Use NN to fit a linear model.\n",
    "* Notice $x = \\mathrm{ReLu}(x) - \\mathrm{ReLu}(-x)$. A linear function can be exactly represented by NN with ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simulate data\n",
    "np.random.seed(1)  # For reproducibility\n",
    "n = 100\n",
    "x = np.random.rand(n, 2)\n",
    "y = 1 + 2 * x[:, 0] + 1 * x[:, 1] + np.random.randn(n)  # Linear relationship with noise\n",
    "y = y.reshape(-1, 1)  # Reshape to column vector (n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden weights\n",
    "        self.b1 = np.zeros((1, hidden_size))  # bias\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output weights\n",
    "        self.b2 = np.zeros((1, output_size))  # bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute the forward pass.\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Input to hidden layer\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Hidden to output layer\n",
    "        self.A2 = self.Z2  # Linear activation (identity)\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Squared Error loss.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Compute gradients using backpropagation.\"\"\"\n",
    "        n = X.shape[0]  # sample size\n",
    "\n",
    "        # Output layer gradients. 2nd layer is computed first.\n",
    "        dZ2 = 2 * (self.A2 - y) / n  # Gradient of loss w.r.t. Z2\n",
    "        \n",
    "        dW2 = np.dot(self.A1.T, dZ2)  # Gradient of loss w.r.t. W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)  # Gradient of loss w.r.t. b2\n",
    "\n",
    "        # Hidden layer gradients. 1st layer is computed last.\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # Gradient of loss w.r.t. A1\n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # Gradient of loss w.r.t. Z1 (ReLU derivative)\n",
    "        dW1 = np.dot(X.T, dZ1)  # Gradient of loss w.r.t. W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)  # Gradient of loss w.r.t. b1\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)  # Forward pass\n",
    "            loss = self.compute_loss(y, y_pred)  # Compute loss\n",
    "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)  # Backward pass\n",
    "            self.update(dW1, db1, dW2, db2, learning_rate)  # Update parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.843827\n",
      "Epoch 100, Loss: 1.589406\n",
      "Epoch 200, Loss: 1.461798\n",
      "Epoch 300, Loss: 1.415684\n",
      "Epoch 400, Loss: 1.347697\n",
      "Epoch 500, Loss: 1.262448\n",
      "Epoch 600, Loss: 1.181384\n",
      "Epoch 700, Loss: 1.124319\n",
      "Epoch 800, Loss: 1.090392\n",
      "Epoch 900, Loss: 1.071762\n",
      "Final Loss: 1.061774\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the neural network\n",
    "    nn = NeuralNetwork(input_size=2, hidden_size=8, output_size=1)\n",
    "    \n",
    "    # Train the network\n",
    "    nn.train(x, y, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Evaluate the final performance\n",
    "    final_pred = nn.forward(x)\n",
    "    final_loss = nn.compute_loss(y, final_pred)\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `pytorch` implementation of the same neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork_torch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 8.963584\n",
      "Epoch 100, Loss: 1.196773\n",
      "Epoch 200, Loss: 1.137686\n",
      "Epoch 300, Loss: 1.098162\n",
      "Epoch 400, Loss: 1.073694\n",
      "Epoch 500, Loss: 1.059587\n",
      "Epoch 600, Loss: 1.051506\n",
      "Epoch 700, Loss: 1.046908\n",
      "Epoch 800, Loss: 1.044397\n",
      "Epoch 900, Loss: 1.043045\n",
      "Final Loss: 1.042303\n"
     ]
    }
   ],
   "source": [
    " # Convert numpy arrays to torch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "model = NeuralNetwork_torch(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate the final performance\n",
    "final_pred = model(x_tensor).detach().numpy()\n",
    "final_loss = criterion(torch.tensor(final_pred), y_tensor).item()\n",
    "print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "* Large $n$\n",
    "* Sample a *minibatch*\n",
    "  * Unbiased gradient, but large variance\n",
    "* Learning rate\n",
    "* Many epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* $L_1$-norm (Lasso)\n",
    "* $L_2$-norm (ridge)\n",
    "* Learning rate\n",
    "* Number of epochs and minibatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "  * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any measurable function.\n",
    "* Deep learning: engineering breakthrough\n",
    "* Big data available"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernel_info": {
   "name": "ir"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
