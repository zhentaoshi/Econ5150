---
title: "Least Squares"
author: Zhentao Shi
format: html
---



Notation: $y_{i}$ is a scalar, and
$x_{i}=\left(x_{i1},\ldots,x_{ip}\right)'$ is a $p\times1$ vector.
$Y=\left(y_{1},\ldots,y_{n}\right)'$ is an $n\times1$ vector, and
$$X=\left[\begin{array}{c}
x_{1}'\\
x_{2}'\\
\vdots\\
x_{n}'
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1p}\\
x_{21} & x_{22} & \cdots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{22} & \cdots & x_{np}
\end{array}\right]$$ is an $n\times p$ matrix. $I_{n}$ is an $n\times n$
identity matrix.

Ordinary least squares (OLS) is the most basic estimation technique in
econometrics. It is simple and transparent. Understanding it thoroughly
paves the way to study more sophisticated linear estimators. Moreover,
many nonlinear estimators resemble the behavior of linear estimators in
a neighborhood of the true value. In this lecture, we learn a series of
facts from the linear algebra operation.

## Estimator

As we have learned from the linear projection model, the projection
coefficient $\beta$ in the regression
$$
\begin{aligned}y & =x'\beta+e\end{aligned}
$${#eq-pop-OLS} 
can be written as
$$\begin{equation}
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].\label{eq:pop_OLS}
\end{equation}$$ We draw a pair of $\left(y,x\right)$ from the joint
distribution, and we mark it as $\left(y_{i},x_{i}\right)$ for
$i=1,\ldots,n$ repeated experiments. We possess a *sample*
$\left(y_{i},x_{i}\right)_{i=1}^{n}$.

::: rem
*1.1*. Is $\left(y_{i},x_{i}\right)$ random or deterministic? Before we
make the observation, they are treated as random variables whose
realized values are uncertain. $\left(y_{i},x_{i}\right)$ is treated as
random when we talk about statistical properties --- statistical
properties of a fixed number is meaningless. After we make the
observation, they become deterministic values which cannot vary anymore.
:::

::: rem
*1.2*. In reality, we have at hand fixed numbers (more recently, words,
photos, audio clips, video clips, etc., which can all be represented in
digital formats with 0 and 1) to feed into a computational operation,
and the operation will return one or some numbers. All statistical
interpretation about these numbers are drawn from the probabilistic
thought experiments. A *thought experiment* is an academic jargon for a
*story* in plain language. Under the axiomatic approach of probability
theory, such stories are mathematical consistent and coherent. But
mathematics is a tautological system, not science. The scientific value
of a probability model depends on how close it is to the *truth* or
implications of the truth. In this course, we suppose that the data are
generated from some mechanism, which is taken as the truth. In the
linear regression model for example, the joint distribution of
$\left(y,x\right)$ is the truth, while we are interested in the linear
projection coefficient $\beta$, which is an implication of the truth as
in (@eq-pop-OLS). Probabilists suppose there is a dragon and try
to tell the dragon's behaviors. Statisticians observe many snakes on
earth, and try to tell what a dragon looks like.
:::

The sample mean is a natural estimator of the population mean. Replace
the population mean $E\left[\cdot\right]$ in(@eq-pop-OLS) by the sample mean
$\frac{1}{n}\sum_{i=1}^{n}\cdot$, and the resulting estimator is
$$\widehat{\beta}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}=\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}=\left(X'X\right)^{-1}X'y$$
if $X'X$ is invertible. This is one way to motivate the OLS estimator.


### Simulation example 
The following code simulates a small dataset and computes OLS using the closed-form formula $\widehat\beta=(X'X)^{-1}X'Y$ (and compares it to `lstsq`).
```{python}
#| label: sim-OLS
#| echo: true

import numpy as np
rng = np.random.default_rng(5150)
n = 10
p = 3  # intercept + 2 regressors
x1 = rng.normal(size=n)
x2 = rng.normal(size=n)
X = np.column_stack([np.ones(n), x1, x2])
beta_true = np.array([1.0, -2.0, 0.5])
eps = rng.normal(scale=0.5, size=n)
y = X @ beta_true + eps
beta_hat = np.linalg.inv(X.T @ X) @ (X.T @ y)
beta_hat_lstsq, *_ = np.linalg.lstsq(X, y, rcond=None)
y_hat = X @ beta_hat
e_hat = y - y_hat
np.set_printoptions(precision=3, suppress=True)
print("X:\n", X)
print("y:\n", y)
print("beta_true:", beta_true)
print("beta_hat (formula):", beta_hat)
print("beta_hat (lstsq)  :", beta_hat_lstsq)
print("X' e_hat (should be ~0):", X.T @ e_hat)
```


## Geometry of OLS

There is natural geometry interpretation of the OLS estimator in a
$n$-dimensional Euclidean space. Notice
$\mathcal{X}=\left\{ Xb:b\in\mathbb{R}^{p}\right\}$ is the linear space
spanned by the $p$ columns of
$X=\left[X_{\cdot1},\ldots,X_{\cdot p}\right]$, which is of
$p$-dimension if the columns are linearly independent. The OLS estimator
is the minimizer of
$\min_{b\in\mathbb{R}^{p}}\left\Vert Y-Xb\right\Vert$ (Square the
Euclidean norm or not does not change the minimizer because $a^{2}$ is a
monotonic transformation for $a\geq 0$). In other words,
$X\widehat{\beta}$ is the point in $\mathcal{X}$ such that it is the
closest to the vector $Y$ in terms of the Euclidean norm.

Define $$\widehat{Y} = X \widehat{\beta} = X (X'X)^{-1}X' Y = P_X Y,$$
where $P_X = X (X'X)^{-1}X'$ is the projector to the columns space of
$X$. One the other hand, define
$$\widehat{e} = Y - \widehat{Y} = (I_n - P_X) Y = P_X^\perp Y,$$ where
$P_X^\perp$ is the projector to the null space of $X$. Since the null
space and the column space are orthogonal, it is easy to verify that
$$\left\langle X\widehat{\beta},\widehat{e}\right\rangle =\widehat{\beta}'X'\widehat{e}=0_{p}^{\prime}.$$
The Pythagorean theorem implies
$$\left\Vert Y\right\Vert ^{2}=\Vert X\widehat{\beta}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.$$

```{python}
#| label: ols-pythag
#| echo: true
y_norm_sq = np.linalg.norm(y) ** 2
yhat_norm_sq = np.linalg.norm(y_hat) ** 2
e_norm_sq = np.linalg.norm(e_hat) ** 2

print("||Y||^2:", y_norm_sq)
print("||X b_hat||^2 + ||e_hat||^2:", yhat_norm_sq + e_norm_sq)
```



If $X$ has full column rank and we use a SVD
$$X=\underset{(n\times n)}{U} \underset{(p\times p)}{S} \underset{(p\times p)}{V}'$$
where $U$ and $V$ have orthonormal columns and $S$ is diagonal with the
$p$ singular values of $X$, then the fitted values can be written as
$$\widehat{Y}=X\widehat{\beta}=X(X'X)^{-1}X'Y=UU'Y.$$ In other words, in
terms of in-sample fitting, OLS using regressors $X$ is equivalent to
regressing $Y$ on the orthonormal basis $U$ for the column space of $X$
(the two procedures have the same projection $\widehat{Y}$). The
corresponding coefficients satisfy $\widehat{\beta}=VS^{-1}U'Y$.

## FWL Theorem

The Frisch-Waugh-Lovell (FWL) theorem is an algebraic fact about the
formula of a subvector of the OLS estimator. Decompose $X = (X_1, X_2)$
where $X_1$ is $n\times (p-1)$ matrix and $X_2$ is an $n\times 1$
vector, so that 
$$\begin{equation}
Y = X_1 \widehat{\beta}_1 + X_2 \widehat{\beta}_2 + \widehat{e}.
\end{equation}
$${#eq-x1x2} 
To find $\widehat{\beta}_2$, we first decompose $X_2$
into its projection onto $\mathrm{span}(X_1)$ and its projection
residual: $$X_2 = P_1 X_2 + P_1^\perp X_2,$$ where
$P_1 = X_1 (X_1' X_1)^{-1} X_1'$ is the projector onto
$\mathrm{span}(X_1)$ and $P_1^\perp=I_n-P_1$. Pre-multiply $P_1^\perp$
on both sides of (@eq-x1x2), we have
$$P_1^\perp Y = P_1^\perp X_1 \widehat{\beta}_1 +  P_1^\perp X_2 \widehat{\beta}_2+P_1^\perp\widehat{e}
= P_1^\perp X_2 \widehat{\beta}_2+ \widehat{e}$$ since $P_1^\perp X_1=0$
and $P_1^\perp \widehat{e} = \widehat{e}$.

Because $\tilde{X}_2 = P_1^\perp X_2$ is orthogonal to
$\mathrm{span}(X_1)$, the coefficient on $X_2$ in the full regression is
pinned down entirely by $\tilde{X}_2$:
$$\widehat{\beta}_2=(\tilde{X}_2'\tilde{X}_2)^{-1}\tilde{X}_2' Y
=(X_2'P_1^\perp X_2)^{-1}X_2'P_1^\perp Y.$$

```{python}
#| label: sim-ols-fwl
#| echo: true

X1 = X[:, 0:2]
X2 = X[:,2]

beta_full, *_ = np.linalg.lstsq(X, y, rcond=None)
beta2_full = beta_full[-1]

P1 = X1 @ np.linalg.inv(X1.T @ X1) @ X1.T
M1 = np.eye(n) - P1
X2_tilde = M1 @ X2
y_tilde = M1 @ y

beta2_fwl = (X2_tilde @ y_tilde) / (X2_tilde @ X2_tilde)

print("beta2 (full):", beta2_full)
print("beta2 (FWL) :", beta2_fwl)
```



## Omitted Variable Bias

Suppose that researcher A runs a full regression
$Y = X_1 \widehat{\beta}_1 + X_2 \widehat{\beta}_2 + \widehat{e}$. When
he sends the data to researcher B, he omits $X_2$ by accident. Research
B can only estimated the coefficient for $X_1$. What would be its
different from researcher A's $\widehat{\beta}_1$?

Notice in researcher A's world: $$\begin{align*}
  Y & = X_1 \widehat{\beta}_1 + X_2 \widehat{\beta}_2 + \widehat{e} \\
    & = X_1 \widehat{\beta}_1 + (P_1X_2+P_1^\perp X_2) \widehat{\beta}_2 + \widehat{e} \\
    & = ( X_1 \widehat{\beta}_1 + P_1X_2 \widehat{\beta}_2 ) + ( P_1^\perp X_2 \widehat{\beta}_2 + \widehat{e}) \\
    & = X_1 (\widehat{\beta}_1 + \widehat{\pi}\widehat{\beta}_2 ) + ( P_1^\perp X_2 \widehat{\beta}_2 + \widehat{e}),
\end{align*}$$ where $\widehat{\pi}=(X_1'X_1)^{-1}X_1'X_2.$ In other
words, researcher B will obtain the coefficient
$(\widehat{\beta}_1 + \widehat{\pi}\widehat{\beta}_2 )$ associated with
$X_1$, and his residual is
$( P_1^\perp X_2 \widehat{\beta}_2 + \widehat{e})$.

Intuitively, regressing on $X_1$ only forces the component
$P_1X_2=X_1\widehat{\pi}$ to be absorbed by the coefficient on $X_1$,
while the orthogonal component $P_1^\perp X_2 \widehat{\beta}_2$ becomes
part of the residual.

## Summary

The derivations above are finite-sample linear algebra in
$\mathbb{R}^n$: $X_1$ and $X_2$ span subspaces of $\mathbb{R}^n$, and
$P_1$ and $P_1^\perp$ are ordinary projection matrices. There are
population counterparts of both the FWL theorem and the omitted variable
bias formula. In that setting, $Y$ and the components of $X$ are random
variables that live in a Hilbert space (typically an $L^2$ space), and
"projection" is defined by the linear projection (orthogonality)
operator rather than an $n\times n$ matrix. The same logic goes through
with population inner products (expectations) replacing sample inner
products, but the geometry is more abstract, so we omit the full
display.

**Historical notes**: Carl Friedrich Gauss (1777--1855) claimed he had
come up with the operation of OLS in 1795. With only three data points
at hand, Gauss successfully applied his method to predict the location
of the dwarf planet Ceres in 1801. While Gauss did not publish the work
on OLS until 1809, Adrien-Marie Legendre (1752--1833) presented this
method in 1805. Today people tend to attribute OLS to Gauss, assuming
that a giant like Gauss had no need to tell a lie to steal Legendre's
discovery.

` Zhentao Shi. 2026-01-15`
