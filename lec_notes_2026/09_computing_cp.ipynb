{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Econ5150: Advanced Computing for Empirical Research\nZhentao Shi\nDepartment of Economics, CUHK\nUpdated: March 2026"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Roadmap\nThis notebook expands the lecture slides into textbook-style notes. We will:\n- motivate why computational skills matter in empirical research;\n- review vectorization and high-performance numerical linear algebra;\n- discuss profiling, memory management, and code optimization;\n- introduce strategies for parallel and distributed computing;\n- outline practical workflows for clusters and cloud resources;\n- emphasize reproducibility through environment management."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 1. Computing in Modern Econometrics\nModern econometric analysis is inseparable from computation. Estimation routines, simulation studies, and empirical policy evaluations all rely on translating mathematical models into efficient code. A naive translation might work for toy datasets, but production-scale projects routinely involve millions of observations, extensive bootstraps, or tuning loops that magnify any inefficiency.\n\nFor graduate-level research the goal is to produce code that is *correct, fast enough, and maintainable*. The rest of these notes develop the toolkit needed to reach that goal, highlighting design principles before diving into specific Python idioms."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Human Time vs. Machine Time\nComputational economics juggles two scarce resources:\n1. **Human time** is spent designing algorithms, reading documentation, and debugging. Optimizing prematurely wastes this resource.\n2. **Machine time** is spent executing code. Ignoring efficiency jeopardizes deadlines when facing large simulations or tight conference submissions.\n\nThe art is to prototype quickly, profile bottlenecks, and then invest effort only where performance matters. The examples below illustrate how structural insight and vectorization deliver sizable gains with limited coding effort."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Illustrative Scale\nLarge-scale empirical projects (for example, Lin, Shi, Wang, and Yan 2022) consume 192 core-hours for a single estimation run (8 wall-clock hours on 24 cores). Such budgets make it infeasible to iterate manually. Thoughtful code architecture -- vectorized kernels, reusable functions, automation for hyperparameter sweeps -- becomes as important as theoretical insight."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 2. Vectorization as the Baseline\nVectorization replaces explicit Python loops with operations implemented in optimized C or Fortran. NumPy, pandas, and SciPy expose these routines through array expressions, broadcasting rules, and linear algebra primitives.\n\nTwo habits help:\n- reason about entire arrays, not element-by-element updates;\n- translate algebraic expressions directly into matrix notation whenever possible.\n\nWhen vectorization is impossible or memory becomes the bottleneck, we fall back to carefully written loops or compiled extensions. Until then, vectorization should be the default mental model."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Case Study: Heteroskedasticity-Robust Variance\nIn OLS with heteroskedastic errors the asymptotic variance of the estimator is\n$$\\sqrt{n}(\\hat\\beta-\\beta_0) \\xrightarrow{d} \\mathcal{N}\\!\\left(0,\\; Q^{-1} \\Omega Q^{-1}\\right),$$\nwhere $Q = \\mathbb{E}[x_i x_i\u0027]$ and\n$$\\Omega = \\mathbb{E}[x_i e_i e_i x_i\u0027] \\approx \\frac{1}{n}\\sum_{i=1}^n x_i x_i\u0027 \\hat e_i^2.$$\n\nEven this simple expression admits multiple computational strategies:\n1. Loop over observations and accumulate $\\hat e_i^2 x_i x_i\u0027$ into a running matrix.\n2. Form $D = \\operatorname{diag}(\\hat e^2)$ and compute $X^\\top D X$ using dense multiplies.\n3. Treat $D$ as sparse to avoid populating off-diagonal zeros.\n4. Compute $X^\\top D^{1/2}$ once and reuse it to form an outer product.\n\nAll formulas agree mathematically, yet their run times differ dramatically depending on the size and sparsity of $X$. The best choice is problem-specific, and profiling guides the decision."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "metadata":  {

                                   },
                      "execution_count":  null,
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import numpy as np\nimport time\n\ndef hac_loop(X, resid):\n    n, k = X.shape\n    sigma = np.zeros((k, k))\n    for i in range(n):\n        xi = X[i]\n        sigma += resid[i] ** 2 * np.outer(xi, xi)\n    return sigma / n\n\ndef hac_vectorized(X, resid):\n    weighted = resid[:, None] * X\n    return weighted.T @ weighted / X.shape[0]\n\ndef hac_einsum(X, resid):\n    weighted = resid[:, None] * X\n    return np.einsum(\u0027ni,nj-\u003eij\u0027, weighted, weighted) / X.shape[0]\n\nrng = np.random.default_rng(20260215)\nX = rng.normal(size=(50_000, 5))\nresid = rng.normal(size=50_000)\n\nstart = time.perf_counter()\n_ = hac_loop(X, resid)\nloop_time = time.perf_counter() - start\n\nstart = time.perf_counter()\n_ = hac_vectorized(X, resid)\nvec_time = time.perf_counter() - start\n\nstart = time.perf_counter()\n_ = hac_einsum(X, resid)\neinsum_time = time.perf_counter() - start\n\nprint(f\u0027loop version:      {loop_time:0.3f} seconds\u0027)\nprint(f\u0027vectorized:        {vec_time:0.3f} seconds\u0027)\nprint(f\u0027einsum/batched:    {einsum_time:0.3f} seconds\u0027)"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "The loop version is easiest to read but pays the price of Python-level iteration. The second implementation treats each residual-weighted regressor as a row of a matrix and relies on highly optimized BLAS routines. The `einsum` formulation makes the contraction explicit and often matches the vectorized implementation while being friendlier to sparse back ends.\n\nWhen comparing implementations, inspect both run time and memory demands. A dense matrix formulation that materializes an $n \\times n$ object is infeasible even if NumPy executes each individual kernel quickly."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 3. Profiling Before Optimizing\nOptimization without measurement is guesswork. Adopt a disciplined workflow:\n1. **Reproducible baseline**: wrap computations in functions and write unit tests or assertions verifying correctness.\n2. **Time the code**: start with `time.perf_counter` or IPython\u0027s `%%timeit` for rough estimates.\n3. **Profile hotspots**: use `cProfile`, `line_profiler`, or `scalene` to identify the functions and lines that dominate execution time.\n4. **Diagnose causes**: determine whether the bottleneck is algorithmic complexity, memory bandwidth, Python overhead, or I/O.\n5. **Iterate deliberately**: apply a single optimization at a time and re-run tests to prevent regressions.\n\nNotebook workflows benefit from storing profiling output (for example in CSV or Markdown tables) so that future readers can understand the performance rationale."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Instrumentation Patterns\n- Wrap expensive loops in helper functions so that profilers can attribute time precisely.\n- Prefer `range` over `while` and local variables over repeated attribute lookups inside loops.\n- Cache pure computations (memoization) only when the hit rate is high; otherwise the bookkeeping costs dominate.\n- When randomness matters, use seeded generators (`np.random.default_rng`) to guarantee reproducibility across optimization attempts."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 4. Memory and Data Layout\nCompute time is irrelevant if the program exhausts memory. Guidelines:\n- Choose suitable data types (`float32`, `int32`) when precision permits; smaller dtypes reduce bandwidth pressure.\n- Use `np.ascontiguousarray` or `np.asarray` to ensure arrays are aligned for BLAS routines.\n- Chunk large jobs: process data in blocks and aggregate partial results to keep peak memory bounded.\n- Exploit sparse structures (`scipy.sparse.csr_matrix`) when matrices contain many zeros.\n- Avoid unnecessary copies; in pandas, prefer vectorized operations over `apply`, and use `assign` or `eval` with `engine=\u0027numexpr\u0027` for arithmetic."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### When Vectorization Is Not Enough\nIf profiling shows Python overhead still dominates, consider:\n- **Numba**: JIT-compile numerical kernels with minimal code changes by decorating functions with `@njit`.\n- **Cython** or **pybind11**: write performant extensions in C/C++ while retaining Python interfaces.\n- **Specialized libraries**: for FFTs, convolutions, or optimization routines, leverage domain-tailored packages before reinventing algorithms.\n\nThe choice depends on how often the code will be reused and the development effort you can justify."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 5. Parallel and Distributed Computing\nParallelism increases throughput by dividing work across cores or machines. Three models appear frequently in empirical research:\n- **Shared-memory parallelism** (threads, multiprocessing) suits moderate-sized tasks on a single workstation.\n- **Task-based schedulers** (joblib, Dask, Ray) coordinate batches of medium-sized jobs, ideal for bootstraps or hyper-parameter sweeps.\n- **Distributed clusters** use multiple nodes connected over a network, managed by schedulers such as SLURM or PBS.\n\nParallel speedups are bounded by Amdahl\u0027s Law: the serial fraction caps the overall gain. Maximize the parallelizable portion by isolating computational kernels and minimizing synchronization."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Choosing a Parallel Strategy\nAsk the following questions before parallelizing:\n1. **Is the workload embarrassingly parallel?** Independent simulations or cross-validation folds are easiest to scale.\n2. **What data must workers access?** Broadcasting large matrices to each process can negate speedups; consider memory-mapped files or shared arrays.\n3. **How will results be aggregated?** Design reducers that combine partial outputs without race conditions.\n4. **Does the environment impose limits?** Desktop operating systems restrict process counts; clusters may require queue submissions with wall-time requests.\n\nFor reproducibility, log the number of workers, random seeds, and per-task run times."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "metadata":  {

                                   },
                      "execution_count":  null,
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import numpy as np\nfrom multiprocessing import Pool, cpu_count\n\ndef _partial_hac(args):\n    X_chunk, resid_chunk = args\n    weighted = resid_chunk[:, None] * X_chunk\n    return weighted.T @ weighted\n\ndef hac_parallel(X, resid, n_jobs=None):\n    if n_jobs is None:\n        n_jobs = max(1, cpu_count() - 1)\n    partitions = np.array_split(np.arange(X.shape[0]), n_jobs)\n    with Pool(processes=n_jobs) as pool:\n        pieces = pool.map(\n            _partial_hac,\n            [(X[idx], resid[idx]) for idx in partitions]\n        )\n    return sum(pieces) / X.shape[0]\n\nif __name__ == \u0027__main__\u0027:\n    rng = np.random.default_rng(20260215)\n    X = rng.normal(size=(200_000, 6))\n    resid = rng.normal(size=200_000)\n    sigma_hat = hac_parallel(X, resid, n_jobs=4)\n    print(\u0027parallel estimate has shape\u0027, sigma_hat.shape)"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "The helper `_partial_hac` computes a partial cross-product; the main function partitions the index set, dispatches tasks to worker processes, and aggregates the results. The `if __name__ == \u0027__main__\u0027:` guard is required on Windows and macOS to prevent infinite process spawning when the module is imported."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 6. Working with Clusters and Cloud Resources\nUniversity clusters and commercial clouds provide elastic compute, but using them efficiently demands planning.\n- **Login nodes** are for file transfers, editing scripts, and light debugging. Heavy computations belong to compute nodes accessed through a scheduler.\n- **Job schedulers** (SLURM, PBS, LSF) require resource requests -- cores, memory, wall time. Under-requesting causes failures; over-requesting lengthens queue time.\n- **Environment setup** is reproducible when scripted: load modules, activate Conda environments, or use container images specified in job scripts.\n- **Monitoring** involves tools like `squeue`, `sacct`, or cloud dashboards to track progress and diagnose errors.\n\nAutomate routine steps with shell scripts so that rerunning an experiment becomes a single command."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Example SLURM Script Skeleton\n```bash\n#!/bin/bash\n#SBATCH --job-name=econ5150-sim\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=logs/%x-%j.out\n\nmodule load python/3.11\nsource activate econ5150\n\npython run_simulation.py --replications 1000 --seed 20260215\n```\nAdjust the directives to match the scheduler on your system (for example, `qsub` for PBS). Always test the script with a short wall-time and few replications before launching large jobs."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Remote Workflow Checklist\n1. **Develop locally**: prototype on small data and ensure scripts accept command-line arguments for inputs, outputs, and random seeds.\n2. **Package dependencies**: export `environment.yml` or `requirements.txt`, or build a Docker or Apptainer image.\n3. **Transfer resources**: use `scp`, `rsync`, or Git to move code and data snapshots to the cluster.\n4. **Submit jobs**: launch batch scripts or array jobs; record job IDs for tracking.\n5. **Collect outputs**: store results in versioned directories, compress large logs, and pull summaries back to your workstation."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 7. Reproducibility and Collaboration\nReproducible computation shortens the path from draft to publication.\n- Maintain a project structure with clear separation between raw data, processed data, code, and figures.\n- Use version control (`git`) to track both code and configuration.\n- Capture software environments with Conda, `requirements.txt`, or container recipes. Record exact package versions in your paper\u0027s appendix.\n- Automate pipelines with Makefiles, `nox`, or simple shell scripts so that a single command can rebuild key tables.\n- Document random seeds and experiment metadata (start time, git commit, hardware) in output files."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Recreating Python Environments\n```bash\n# create an environment from a specification file\nconda env create -f environment.yml\nconda activate econ5150\n\n# record the environment after installing additional packages\nconda env export --from-history \u003e environment.yml\n```\nFor containerized workflows, adapt these commands in a `Dockerfile` or `apptainer.def` so that collaborators can launch identical environments on any machine."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## 8. Key Takeaways\n- Start with clear mathematical expressions; translate them into vectorized code that mirrors the algebra.\n- Measure performance before optimizing, and optimize only the true bottlenecks.\n- Treat memory as a first-class constraint and design data flows that scale gracefully.\n- Parallel computation is powerful when the workload is partitionable and aggregation is cheap.\n- Reproducible workflows -- environment management, logging, automation -- turn computational experiments into reliable scientific evidence."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Further Reading\n- McKinney (2022), *Python for Data Analysis*, chapters on performance and vectorization.\n- van der Walt, Colbert, and Varoquaux (2011), *The NumPy Array: A Structure for Efficient Numerical Computation*.\n- Gorelick and Ozsvald (2020), *High Performance Python*, for profiling and parallel patterns.\n- Documentation for your cluster\u0027s scheduler and the CUHK Econ computing resources portal."
                                 ]
                  }
              ],
    "metadata":  {
                     "celltoolbar":  "Slideshow",
                     "kernelspec":  {
                                        "display_name":  "Python 3 (ipykernel)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.11.3"
                                       },
                     "rise":  {
                                  "enable_chalkboard":  true,
                                  "scroll":  true,
                                  "theme":  "serif"
                              }
                 },
    "nbformat":  4,
    "nbformat_minor":  2
}
