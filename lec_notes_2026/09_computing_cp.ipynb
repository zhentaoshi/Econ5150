{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Econ5150: Advanced Computing\n",
                "Zhentao Shi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "Modern econometric analysis is inseparable from computation. Estimation routines, simulation studies, and empirical policy evaluations all rely on translating mathematical models into efficient code. A naive translation might work for toy datasets, but production-scale projects routinely involve large datasets and tuning loops that magnify any inefficiency."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "For research the goal is to produce code that is correct, fast enough, and maintainable.  \n",
                "As an illustrative example, Lin, Shi, Wang, and Yan (2022)'s large-scale empirical projects consume 192 core-hours for a single estimation run, while it takes 8 wall-clock hours on 24 cores ($192 = 8 \\times 24$). Such budgets make it infeasible to iterate manually. Thoughtful code architecture -- vectorized kernels, reusable functions, automation for hyperparameter sweeps are important.\n",
                "\n",
                "\n",
                "Computational economics juggles two scarce resources:\n",
                "**Human time** is spent designing algorithms, reading documentation, and debugging. Optimizing prematurely wastes this resource.\n",
                "**Machine time** is spent executing code. Ignoring efficiency jeopardizes deadlines when facing large simulations.\n",
                "\n",
                "The art is to prototype quickly, profile bottlenecks, and then invest effort only where performance matters."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Vectorization\n",
                "\n",
                "Vectorization replaces explicit Python loops with operations implemented in lower-level languages. NumPy, pandas, and SciPy explore these routines through array expressions, broadcasting rules, and linear algebra primitives.\n",
                "\n",
                " \n",
                "Nowadays with vibe coding, AI is likely to follow the mathematical expressions, instead of reasoning by itself about the choices. \n",
                "Although vectorization is in general encouraged, if linear algebra is the bottleneck, careful testing will be essential for the performance of the code. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Case Study: Heteroskedasticity-Robust Variance\n",
                "In OLS with heteroskedastic errors the asymptotic variance of the estimator is\n",
                "$$\\sqrt{n}(\\hat\\beta-\\beta_0) \\xrightarrow{d} \\mathcal{N}\\!\\left(0,\\; Q^{-1} \\Omega Q^{-1}\\right),$$\n",
                "where $Q = \\mathbb{E}[x_i x_i']$ and\n",
                "$\\Omega = \\mathbb{E}[x_i e_i e_i x_i']$. Its sample analog has the following expressions are equivalent:\n",
                "$$\n",
                "\\underset{\\mathrm{opt1}}{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\widehat{e}_{i}^{2}}=\\underset{\\mathrm{opt2,3}}{\\frac{1}{n}X'DX}=\\underset{\\mathrm{opt 4}}{\\frac{1}{n}\\left(X'D^{1/2}\\right)\\left(D^{1/2}X\\right)}\n",
                "$$\n",
                "where $D$ is a diagonal matrix of $\\left(\\widehat{e}_{1}^{2},\\widehat{e}_{2,}^{2},\\ldots,\\widehat{e}_{n}^{2}\\right)$. \n",
                "It admits multiple computational strategies:\n",
                "1. Loop over observations and accumulate $\\hat e_i^2 x_i x_i'$ into a running matrix.\n",
                "2. Form $D = \\operatorname{diag}(\\hat e^2)$ and compute $X^\\top D X$ using dense multiplies.\n",
                "3. Treat $D$ as sparse to avoid populating off-diagonal zeros.\n",
                "4. Compute $X^\\top D^{1/2}$ once and reuse it to form an outer product.\n",
                "\n",
                "All formulas agree mathematically, yet their run times differ dramatically depending on the size and sparsity of $X$. The best choice is problem-specific, and profiling guides the decision."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import time\n",
                "\n",
                "def hac_loop(X, resid):\n",
                "    n, k = X.shape\n",
                "    sigma = np.zeros((k, k))\n",
                "    for i in range(n):\n",
                "        xi = X[i]\n",
                "        sigma += resid[i] ** 2 * np.outer(xi, xi)\n",
                "    return sigma / n\n",
                "\n",
                "def hac_vectorized(X, resid):\n",
                "    weighted = resid[:, None] * X\n",
                "    return weighted.T @ weighted / X.shape[0]\n",
                "\n",
                "def hac_einsum(X, resid):\n",
                "    weighted = resid[:, None] * X\n",
                "    return np.einsum('ni,nj->ij', weighted, weighted) / X.shape[0]\n",
                "\n",
                "rng = np.random.default_rng(20260215)\n",
                "X = rng.normal(size=(50_000, 5))\n",
                "resid = rng.normal(size=50_000)\n",
                "\n",
                "start = time.perf_counter()\n",
                "_ = hac_loop(X, resid)\n",
                "loop_time = time.perf_counter() - start\n",
                "\n",
                "start = time.perf_counter()\n",
                "_ = hac_vectorized(X, resid)\n",
                "vec_time = time.perf_counter() - start\n",
                "\n",
                "start = time.perf_counter()\n",
                "_ = hac_einsum(X, resid)\n",
                "einsum_time = time.perf_counter() - start\n",
                "\n",
                "print(f'loop version:      {loop_time:0.3f} seconds')\n",
                "print(f'vectorized:        {vec_time:0.3f} seconds')\n",
                "print(f'einsum/batched:    {einsum_time:0.3f} seconds')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The loop version is easiest to read but pays the price of Python-level iteration. The second implementation treats each residual-weighted regressor as a row of a matrix and relies on highly optimized BLAS routines. The `einsum` formulation makes the contraction explicit and often matches the vectorized implementation while being friendlier to sparse back ends.\n",
                "\n",
                "When comparing implementations, inspect both run time and memory demands. A dense matrix formulation that materializes an $n \\times n$ object is infeasible even if NumPy executes each individual kernel quickly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Profiling Before Optimizing\n",
                "Optimization without measurement is guesswork. Adopt a disciplined workflow:\n",
                "1. **Reproducible baseline**: wrap computations in functions and write unit tests or assertions verifying correctness.\n",
                "2. **Time the code**: start with `time.perf_counter` or IPython's `%%timeit` for rough estimates.\n",
                "3. **Profile hotspots**: use `cProfile`, `line_profiler`, or `scalene` to identify the functions and lines that dominate execution time.\n",
                "4. **Diagnose causes**: determine whether the bottleneck is algorithmic complexity, memory bandwidth, Python overhead, or I/O.\n",
                "5. **Iterate deliberately**: apply a single optimization at a time and re-run tests to prevent regressions.\n",
                "\n",
                "Notebook workflows benefit from storing profiling output (for example in CSV or Markdown tables) so that future readers can understand the performance rationale."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Instrumentation Patterns\n",
                "- Wrap expensive loops in helper functions so that profilers can attribute time precisely.\n",
                "- Prefer `range` over `while` and local variables over repeated attribute lookups inside loops.\n",
                "- Cache pure computations (memoization) only when the hit rate is high; otherwise the bookkeeping costs dominate.\n",
                "- When randomness matters, use seeded generators (`np.random.default_rng`) to guarantee reproducibility across optimization attempts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Memory and Data Layout\n",
                "Compute time is irrelevant if the program exhausts memory. Guidelines:\n",
                "- Choose suitable data types (`float32`, `int32`) when precision permits; smaller dtypes reduce bandwidth pressure.\n",
                "- Use `np.ascontiguousarray` or `np.asarray` to ensure arrays are aligned for BLAS routines.\n",
                "- Chunk large jobs: process data in blocks and aggregate partial results to keep peak memory bounded.\n",
                "- Exploit sparse structures (`scipy.sparse.csr_matrix`) when matrices contain many zeros.\n",
                "- Avoid unnecessary copies; in pandas, prefer vectorized operations over `apply`, and use `assign` or `eval` with `engine='numexpr'` for arithmetic."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### When Vectorization Is Not Enough\n",
                "If profiling shows Python overhead still dominates, consider:\n",
                "- **Numba**: JIT-compile numerical kernels with minimal code changes by decorating functions with `@njit`.\n",
                "- **Cython** or **pybind11**: write performant extensions in C/C++ while retaining Python interfaces.\n",
                "- **Specialized libraries**: for FFTs, convolutions, or optimization routines, leverage domain-tailored packages before reinventing algorithms.\n",
                "\n",
                "The choice depends on how often the code will be reused and the development effort you can justify."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Parallel and Distributed Computing\n",
                "Parallelism increases throughput by dividing work across cores or machines. Three models appear frequently in empirical research:\n",
                "- **Shared-memory parallelism** (threads, multiprocessing) suits moderate-sized tasks on a single workstation.\n",
                "- **Task-based schedulers** (joblib, Dask, Ray) coordinate batches of medium-sized jobs, ideal for bootstraps or hyper-parameter sweeps.\n",
                "- **Distributed clusters** use multiple nodes connected over a network, managed by schedulers such as SLURM or PBS.\n",
                "\n",
                "Parallel speedups are bounded by Amdahl's Law: the serial fraction caps the overall gain. Maximize the parallelizable portion by isolating computational kernels and minimizing synchronization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Choosing a Parallel Strategy\n",
                "Ask the following questions before parallelizing:\n",
                "1. **Is the workload embarrassingly parallel?** Independent simulations or cross-validation folds are easiest to scale.\n",
                "2. **What data must workers access?** Broadcasting large matrices to each process can negate speedups; consider memory-mapped files or shared arrays.\n",
                "3. **How will results be aggregated?** Design reducers that combine partial outputs without race conditions.\n",
                "4. **Does the environment impose limits?** Desktop operating systems restrict process counts; clusters may require queue submissions with wall-time requests.\n",
                "\n",
                "For reproducibility, log the number of workers, random seeds, and per-task run times."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from multiprocessing import Pool, cpu_count\n",
                "\n",
                "def _partial_hac(args):\n",
                "    X_chunk, resid_chunk = args\n",
                "    weighted = resid_chunk[:, None] * X_chunk\n",
                "    return weighted.T @ weighted\n",
                "\n",
                "def hac_parallel(X, resid, n_jobs=None):\n",
                "    if n_jobs is None:\n",
                "        n_jobs = max(1, cpu_count() - 1)\n",
                "    partitions = np.array_split(np.arange(X.shape[0]), n_jobs)\n",
                "    with Pool(processes=n_jobs) as pool:\n",
                "        pieces = pool.map(\n",
                "            _partial_hac,\n",
                "            [(X[idx], resid[idx]) for idx in partitions]\n",
                "        )\n",
                "    return sum(pieces) / X.shape[0]\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    rng = np.random.default_rng(20260215)\n",
                "    X = rng.normal(size=(200_000, 6))\n",
                "    resid = rng.normal(size=200_000)\n",
                "    sigma_hat = hac_parallel(X, resid, n_jobs=4)\n",
                "    print('parallel estimate has shape', sigma_hat.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The helper `_partial_hac` computes a partial cross-product; the main function partitions the index set, dispatches tasks to worker processes, and aggregates the results. The `if __name__ == '__main__':` guard is required on Windows and macOS to prevent infinite process spawning when the module is imported."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Working with Clusters and Cloud Resources\n",
                "University clusters and commercial clouds provide elastic compute, but using them efficiently demands planning.\n",
                "- **Login nodes** are for file transfers, editing scripts, and light debugging. Heavy computations belong to compute nodes accessed through a scheduler.\n",
                "- **Job schedulers** (SLURM, PBS, LSF) require resource requests -- cores, memory, wall time. Under-requesting causes failures; over-requesting lengthens queue time.\n",
                "- **Environment setup** is reproducible when scripted: load modules, activate Conda environments, or use container images specified in job scripts.\n",
                "- **Monitoring** involves tools like `squeue`, `sacct`, or cloud dashboards to track progress and diagnose errors.\n",
                "\n",
                "Automate routine steps with shell scripts so that rerunning an experiment becomes a single command."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example SLURM Script Skeleton\n",
                "```bash\n",
                "#!/bin/bash\n",
                "#SBATCH --job-name=econ5150-sim\n",
                "#SBATCH --nodes=1\n",
                "#SBATCH --ntasks=1\n",
                "#SBATCH --cpus-per-task=8\n",
                "#SBATCH --mem=32G\n",
                "#SBATCH --time=04:00:00\n",
                "#SBATCH --output=logs/%x-%j.out\n",
                "\n",
                "module load python/3.11\n",
                "source activate econ5150\n",
                "\n",
                "python run_simulation.py --replications 1000 --seed 20260215\n",
                "```\n",
                "Adjust the directives to match the scheduler on your system (for example, `qsub` for PBS). Always test the script with a short wall-time and few replications before launching large jobs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Remote Workflow Checklist\n",
                "1. **Develop locally**: prototype on small data and ensure scripts accept command-line arguments for inputs, outputs, and random seeds.\n",
                "2. **Package dependencies**: export `environment.yml` or `requirements.txt`, or build a Docker or Apptainer image.\n",
                "3. **Transfer resources**: use `scp`, `rsync`, or Git to move code and data snapshots to the cluster.\n",
                "4. **Submit jobs**: launch batch scripts or array jobs; record job IDs for tracking.\n",
                "5. **Collect outputs**: store results in versioned directories, compress large logs, and pull summaries back to your workstation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Reproducibility and Collaboration\n",
                "Reproducible computation shortens the path from draft to publication.\n",
                "- Maintain a project structure with clear separation between raw data, processed data, code, and figures.\n",
                "- Use version control (`git`) to track both code and configuration.\n",
                "- Capture software environments with Conda, `requirements.txt`, or container recipes. Record exact package versions in your paper's appendix.\n",
                "- Automate pipelines with Makefiles, `nox`, or simple shell scripts so that a single command can rebuild key tables.\n",
                "- Document random seeds and experiment metadata (start time, git commit, hardware) in output files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Recreating Python Environments\n",
                "```bash\n",
                "# create an environment from a specification file\n",
                "conda env create -f environment.yml\n",
                "conda activate econ5150\n",
                "\n",
                "# record the environment after installing additional packages\n",
                "conda env export --from-history > environment.yml\n",
                "```\n",
                "For containerized workflows, adapt these commands in a `Dockerfile` or `apptainer.def` so that collaborators can launch identical environments on any machine."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "- Start with clear mathematical expressions; translate them into vectorized code that mirrors the algebra.\n",
                "- Measure performance before optimizing, and optimize only the true bottlenecks.\n",
                "- Treat memory as a first-class constraint and design data flows that scale gracefully.\n",
                "- Parallel computation is powerful when the workload is partitionable and aggregation is cheap.\n",
                "- Reproducible workflows -- environment management, logging, automation -- turn computational experiments into reliable scientific evidence."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Further Reading\n",
                "- McKinney (2022), *Python for Data Analysis*, chapters on performance and vectorization.\n",
                "- van der Walt, Colbert, and Varoquaux (2011), *The NumPy Array: A Structure for Efficient Numerical Computation*.\n",
                "- Gorelick and Ozsvald (2020), *High Performance Python*, for profiling and parallel patterns.\n",
                "- Documentation for your cluster's scheduler and the CUHK Econ computing resources portal."
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        },
        "rise": {
            "enable_chalkboard": true,
            "scroll": true,
            "theme": "serif"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
