#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
\end_modules
\maintain_unincluded_children no
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize 12
\spacing single
\use_hyperref false
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{2}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Least Squares 
\end_layout

\begin_layout Standard
Notation:
 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar,
 and 
\begin_inset Formula $x_{i}=\left(x_{i1},\ldots,x_{iK}\right)'$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y=\left(y_{1},\ldots,y_{n}\right)'$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector,
 and 
\begin_inset Formula 
\[
X=\left[\begin{array}{c}
x_{1}'\\
x_{2}'\\
\vdots\\
x_{n}'
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1K}\\
x_{21} & x_{22} & \cdots & x_{2K}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{22} & \cdots & x_{nK}
\end{array}\right]
\]

\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
 
\begin_inset Formula $I_{n}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 identity matrix.
\end_layout

\begin_layout Standard
Ordinary least squares (OLS) is the most basic estimation technique in econometrics.
 It is simple and transparent.
 Understanding it thoroughly paves the way to study more sophisticated linear estimators.
 Moreover,
 many nonlinear estimators resemble the behavior of linear estimators in a neighborhood of the true value.
 In this lecture,
 we learn a series of facts from the linear algebra operation.
 
\end_layout

\begin_layout Section
Estimator
\end_layout

\begin_layout Standard
As we have learned from the linear projection model,
 the projection coefficient 
\begin_inset Formula $\beta$
\end_inset

 in the regression 
\begin_inset Formula 
\[
\begin{aligned}y & =x'\beta+e\end{aligned}
\]

\end_inset

can be written as 
\begin_inset Formula 
\begin{equation}
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].\label{eq:pop_OLS}
\end{equation}

\end_inset

 We draw a pair of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 from the joint distribution,
 and we mark it as 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 for 
\begin_inset Formula $i=1,\ldots,n$
\end_inset

 repeated experiments.
 We possess a 
\emph on
sample
\emph default
 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Remark
Is 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 random or deterministic?
 Before we make the observation,
 they are treated as random variables whose realized values are uncertain.
 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is treated as random when we talk about statistical properties —
 statistical properties of a fixed number is meaningless.
 After we make the observation,
 they become deterministic values which cannot vary anymore.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
In reality,
 we have at hand fixed numbers (more recently,
 words,
 photos,
 audio clips,
 video clips,
 etc.,
 which can all be represented in digital formats with 0 and 1) to feed into a computational operation,
 and the operation will return one or some numbers.
 All statistical interpretation about these numbers are drawn from the probabilistic thought experiments.
 A 
\emph on
thought experiment
\emph default
 is an academic jargon for a 
\emph on
story
\emph default
 in plain language.
 Under the axiomatic approach of probability theory,
 such stories are mathematical consistent and coherent.
 But mathematics is a tautological system,
 not science.
 The scientific value of a probability model depends on how close it is to the 
\emph on
truth
\emph default
 or implications of the truth.
 In this course,
 we suppose that the data are generated from some mechanism,
 which is taken as the truth.
 In the linear regression model for example,
 the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 is the truth,
 while we are interested in the linear projection coefficient 
\begin_inset Formula $\beta$
\end_inset

,
 which is an implication of the truth as in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
 Probabilists suppose there is a dragon and try to tell the dragon's behaviors.
 Statisticians observe many snakes on earth,
 and try to tell what a dragon looks like.
\end_layout

\begin_layout Standard
The sample mean is a natural estimator of the population mean.
 Replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) by the sample mean 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\cdot$
\end_inset

,
 and the resulting estimator is 
\begin_inset Formula 
\[
\widehat{\beta}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}=\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}=\left(X'X\right)^{-1}X'y
\]

\end_inset

if 
\begin_inset Formula $X'X$
\end_inset

 is invertible.
 This is one way to motivate the OLS estimator.
 
\end_layout

\begin_layout Standard
The operation of OLS bears a natural geometric interpretation.
 Notice 
\begin_inset Formula $\mathcal{X}=\left\{ Xb:b\in\mathbb{R}^{K}\right\} $
\end_inset

 is the linear space spanned by the 
\begin_inset Formula $K$
\end_inset

 columns of 
\begin_inset Formula $X=\left[X_{\cdot1},\ldots,X_{\cdot K}\right]$
\end_inset

,
 which is of 
\begin_inset Formula $K$
\end_inset

-dimension if the columns are linearly independent.
 The OLS estimator is the minimizer of 
\begin_inset Formula $\min_{b\in\mathbb{R}^{K}}\left\Vert Y-Xb\right\Vert $
\end_inset

 (Square the Euclidean norm or not does not change the minimizer because 
\begin_inset Formula $a^{2}$
\end_inset

 is a monotonic transformation for 
\begin_inset Formula $a\geq0$
\end_inset

).
 In other words,
 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the point in 
\begin_inset Formula $\mathcal{X}$
\end_inset

 such that it is the closest to the vector 
\begin_inset Formula $Y$
\end_inset

 in terms of the Euclidean norm.
\end_layout

\begin_layout Standard
The relationship 
\begin_inset Formula $Y=X\widehat{\beta}+\widehat{e}$
\end_inset

 decomposes 
\begin_inset Formula $Y$
\end_inset

 into two orthogonal vectors 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 and 
\begin_inset Formula $\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\left\langle X\widehat{\beta},\widehat{e}\right\rangle =\widehat{\beta}'X'\widehat{e}=0_{K}^{\prime}$
\end_inset

,
 where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is the 
\emph on
inner product
\emph default
 of two vectors.
 Therefore 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the 
\emph on
projection
\emph default
 of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $\mathcal{X}$
\end_inset

,
 and 
\begin_inset Formula $\widehat{e}$
\end_inset

 is the corresponding 
\emph on
projection residuals.
 
\emph default
The Pythagorean theorem implies 
\begin_inset Formula $\left\Vert Y\right\Vert ^{2}=\Vert X\widehat{\beta}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.$
\end_inset

 
\end_layout

\begin_layout Section
Subvector
\end_layout

\begin_layout Standard
The Frisch-Waugh-Lovell (FWL) theorem is an algebraic fact about the formula of a subvector of the OLS estimator.
 To derive the FWL theorem we need to use the inverse of partitioned matrix.
 For a positive definite symmetric matrix 
\begin_inset Formula $A=\begin{pmatrix}A_{11} & A_{12}\\
A_{12}' & A_{22}
\end{pmatrix}$
\end_inset

,
 the inverse can be written as 
\begin_inset Formula 
\[
A^{-1}=\begin{pmatrix}\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & -\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1}A_{12}A_{22}^{-1}\\
-A_{22}^{-1}A_{12}'\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & \left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset

In our context of OLS estimator,
 let 
\begin_inset Formula $X=\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\begin{pmatrix}\widehat{\beta}_{1}\\
\widehat{\beta}_{2}
\end{pmatrix} & =\widehat{\beta}=(X'X)^{-1}X'Y\\
 & =\left(\begin{pmatrix}X_{1}'\\
X_{2}'
\end{pmatrix}\begin{pmatrix}X_{1} & X_{2}\end{pmatrix}\right)^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\
X_{2}'X_{1} & X_{2}'X_{2}
\end{pmatrix}^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1} & -\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}\\
\heartsuit & \heartsuit
\end{pmatrix}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The subvector
\begin_inset Formula 
\begin{align*}
\widehat{\beta}_{1} & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}X_{2}'Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'P_{X_{2}}Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\right)\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 can be obtained by the following:
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

,
 obtain the residual 
\begin_inset Formula $\tilde{Y}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

,
 obtain the residual 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\tilde{Y}$
\end_inset

 on 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

,
 obtain OLS estimates 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
Similar derivation can also be carried out in the population linear projection.
 See Hansen (2020) [E] Chapter 2.22-23.
\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard

\series bold
Historical notes
\series default
:
 Carl Friedrich Gauss (1777–1855) claimed he had come up with the operation of OLS in 1795.
 With only three data points at hand,
 Gauss successfully applied his method to predict the location of the dwarf planet Ceres in 1801.
 While Gauss did not publish the work on OLS until 1809,
 Adrien-Marie Legendre (1752–1833) presented this method in 1805.
 Today people tend to attribute OLS to Gauss,
 assuming that a giant like Gauss had no need to tell a lie to steal Legendre's discovery.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 
\backslash
today}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
